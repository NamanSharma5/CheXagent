{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_id', 'patch_embeddings', 'post_layer_norm', 'q_former',\n",
      "       'language_projection', 'Infiltration', 'Pleural effusion',\n",
      "       'Consolidation', 'Clavicle fracture', 'Aortic enlargement',\n",
      "       'Enlarged PA', 'Pulmonary fibrosis', 'Lung cavity', 'Nodule/Mass',\n",
      "       'Other lesion', 'Rib fracture', 'Lung Opacity', 'Atelectasis',\n",
      "       'Calcification', 'Pleural thickening', 'Lung cyst', 'No finding',\n",
      "       'Emphysema', 'ILD', 'Pneumothorax', 'Mediastinal shift', 'Cardiomegaly',\n",
      "       'Infiltration position', 'Pleural effusion position',\n",
      "       'Consolidation position', 'Clavicle fracture position',\n",
      "       'Aortic enlargement position', 'Enlarged PA position',\n",
      "       'Pulmonary fibrosis position', 'Lung cavity position',\n",
      "       'Nodule/Mass position', 'Other lesion position',\n",
      "       'Rib fracture position', 'Lung Opacity position',\n",
      "       'Atelectasis position', 'Calcification position',\n",
      "       'Pleural thickening position', 'Lung cyst position',\n",
      "       'No finding position', 'Emphysema position', 'ILD position',\n",
      "       'Pneumothorax position', 'Mediastinal shift position',\n",
      "       'Cardiomegaly position'],\n",
      "      dtype='object')\n",
      "Index(['image_id', 'patch_embeddings', 'post_layer_norm', 'q_former',\n",
      "       'language_projection', 'left Infiltration', 'right Infiltration',\n",
      "       'left Pleural effusion', 'right Pleural effusion', 'left Consolidation',\n",
      "       'right Consolidation', 'left Clavicle fracture',\n",
      "       'right Clavicle fracture', 'left Aortic enlargement',\n",
      "       'right Aortic enlargement', 'left Enlarged PA', 'right Enlarged PA',\n",
      "       'left Pulmonary fibrosis', 'right Pulmonary fibrosis',\n",
      "       'left Lung cavity', 'right Lung cavity', 'left Nodule/Mass',\n",
      "       'right Nodule/Mass', 'left Other lesion', 'right Other lesion',\n",
      "       'left Rib fracture', 'right Rib fracture', 'left Lung Opacity',\n",
      "       'right Lung Opacity', 'left Atelectasis', 'right Atelectasis',\n",
      "       'left Calcification', 'right Calcification', 'left Pleural thickening',\n",
      "       'right Pleural thickening', 'left Lung cyst', 'right Lung cyst',\n",
      "       'left Emphysema', 'right Emphysema', 'left ILD', 'right ILD',\n",
      "       'left Pneumothorax', 'right Pneumothorax', 'left Mediastinal shift',\n",
      "       'right Mediastinal shift', 'left Cardiomegaly', 'right Cardiomegaly'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# print the columns of the following dataframes\n",
    "collated_dataframe_path = Path(\"/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/model_inspection/embeddings/no_tuning_4934e91451945c8218c267aae9c34929a7677829/collated_dataframe.pkl\")\n",
    "test_VinDr_path = Path(\"/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/model_inspection/embeddings/no_tuning_4934e91451945c8218c267aae9c34929a7677829/test_VinDr_df_for_training_layer.pkl\")\n",
    "\n",
    "collated_dataframe = pd.read_pickle(collated_dataframe_path)\n",
    "test_VinDr = pd.read_pickle(test_VinDr_path)\n",
    "\n",
    "print(collated_dataframe.columns)\n",
    "print(test_VinDr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedResidualLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, attention_heads=1 ,fine_grained_gating=False):\n",
    "        super().__init__()\n",
    "        # self.fc = nn.Linear(hidden_size, hidden_size) # avoiding linear layer for now\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, attention_heads)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.fine_grained_gating = fine_grained_gating\n",
    "        if fine_grained_gating:\n",
    "            # When fine-grained gating is True, initialize a gate with one parameter per dimension\n",
    "            self.gate = nn.Parameter(torch.zeros(hidden_size))\n",
    "        else:\n",
    "            # When fine-grained gating is False, use a single scalar parameter for gating\n",
    "            self.gate = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # dimensions are [batch_size,1,128,768] change to [batch_size,128,768]\n",
    "        x = x.squeeze(1)\n",
    "        residual = x\n",
    "        x = self.attention(x, x, x)[0]  \n",
    "        x = F.gelu(x)\n",
    "        x = self.layer_norm(x)\n",
    "        if self.fine_grained_gating:\n",
    "            # Apply fine-grained gating by multiplying x with the gate vector (element-wise)\n",
    "            x = x * torch.sigmoid(self.gate) + residual\n",
    "        else:\n",
    "            x = x * self.gate + residual\n",
    "            \n",
    "        x = x.unsqueeze(1)\n",
    "        return x\n",
    "\n",
    "class PredictionHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(PredictionHead, self).__init__()\n",
    "        # Global Average Pooling across the sequence dimension\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        # Optional: Additional linear layers can be added here for more complexity\n",
    "        self.fc1 = nn.Linear(input_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # dimensions are [batch_size,1,128,768] change to [batch_size,128,768]\n",
    "        x = x.squeeze(1)\n",
    "        # Applying global average pooling\n",
    "        x = x.transpose(1, 2) # Change to [batch_size, 768, 128] to match pooling dimension\n",
    "        x = self.gap(x).squeeze() # After pooling, size: [batch_size, 768]\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model def: join together the GatedResidualLayer and PredictionHead\n",
    "class AddedGatedResidualLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes, attention_heads=1, fine_grained_gating=False):\n",
    "        super().__init__()\n",
    "        self.gated_residual_layer = GatedResidualLayer(hidden_size)\n",
    "        self.prediction_head = PredictionHead(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.gated_residual_layer(x)\n",
    "        x = self.prediction_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model def: Stacked Added Gated Residual Layer\n",
    "class StackedAddedGatedResidualLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes, attention_heads=1, fine_grained_gating=False):\n",
    "        super().__init__()\n",
    "        self.gated_residual_layer = GatedResidualLayer(hidden_size)\n",
    "        self.gated_residual_layer2 = GatedResidualLayer(hidden_size)\n",
    "        self.prediction_head = PredictionHead(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.gated_residual_layer(x)\n",
    "        x = self.gated_residual_layer2(x)\n",
    "        x = self.prediction_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_df_for_training_layer_path = Path('/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/model_inspection/embeddings/no_tuning_4934e91451945c8218c267aae9c34929a7677829/test_VinDr_df_for_training_layer.pkl')\n",
    "df = pd.read_pickle(stored_df_for_training_layer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VinDrWithBinaryPathologyLocations(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        # Extract just the columns that indicate pathologies (left or right in their names)\n",
    "        self.pathology_columns = [col for col in dataframe.columns if 'left' in col or 'right' in col]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        \n",
    "        # Extract embeddings and drop the first dimension i.e. [1,128,768] -> [128,768]\n",
    "        embeddings = torch.tensor(row['q_former'], dtype=torch.float)\n",
    "        \n",
    "        # Extract binary labels for pathologies\n",
    "        labels_values = row[self.pathology_columns].values\n",
    "        # set the dtype of each item in values to be an int\n",
    "        labels_values = [int(item) for item in labels_values]\n",
    "        labels_tensor = torch.tensor(labels_values, dtype=torch.float)\n",
    "\n",
    "        # image_id \n",
    "        image_id = row['image_id']\n",
    "        \n",
    "        return image_id, embeddings, labels_tensor\n",
    "    \n",
    "    @staticmethod\n",
    "    def proportions_of_positives_vs_negatives(dataset):\n",
    "        # go through dataset and calculate total no of 0s and 1s in the labels\n",
    "        total_0s = 0\n",
    "        total_1s = 0\n",
    "        for i in range(len(dataset)):\n",
    "            _, _, labels = dataset[i]\n",
    "            total_0s += len(labels) - labels.sum()\n",
    "            total_1s += labels.sum()\n",
    "        return total_0s, total_1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Implements Focal Loss.\n",
    "        Args:\n",
    "        - alpha (float): Weighting factor for the binary class at index 1 (default is 0.25).\n",
    "        - gamma (float): Focusing parameter to adjust the rate at which easy examples are down-weighted (default is 2).\n",
    "        - reduction (str): Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Compute the focal loss given the model output (inputs) and the target labels.\n",
    "        Args:\n",
    "        - inputs (Tensor): Predictions from model (logits before Sigmoid).\n",
    "        - targets (Tensor): Target labels.\n",
    "        \"\"\"\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        targets = targets.type(torch.float32)\n",
    "        at = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = at * (1 - pt) ** self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return F_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return F_loss.sum()\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data hyperparams\n",
    "training_proportion = 0.8\n",
    "validation_proportion = 0.1\n",
    "batch_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VinDrWithBinaryPathologyLocations(df)\n",
    "# total_0s,total_1s = VinDrWithBinaryPathologyLocations.proportions_of_positives_vs_negatives(dataset)\n",
    "# print(f'Proportion of 0s: {total_0s/(total_0s+total_1s)}, Proportion of 1s: {total_1s/(total_0s+total_1s)}')\n",
    "\n",
    "# split into training, validation and test sets with 0.8, 0.1, 0.1 , then create dataloaders from these\n",
    "train_size = int(training_proportion * len(dataset))\n",
    "val_size = int(validation_proportion * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(outputs, labels):\n",
    "    # Apply sigmoid to get predicted probabilities \n",
    "    predictions = torch.sigmoid(outputs)  \n",
    "\n",
    "    # Threshold for determining positive predictions (adjust as needed)\n",
    "    predictions = predictions > 0.5   \n",
    "   \n",
    "\n",
    "    # Calculate metrics row-wise, considering all pathologies\n",
    "    accuracy = (predictions == labels).all(dim=1).float().mean()  \n",
    "    # Avoid division by zero using clamp\n",
    "    true_positives = torch.logical_and(predictions, labels).float().sum(dim=1)\n",
    "    predicted_positives = predictions.sum(dim=1).float()\n",
    "    actual_positives = labels.sum(dim=1).float()\n",
    "    \n",
    "    precision = true_positives / predicted_positives.clamp(min=1)\n",
    "    recall = true_positives / actual_positives.clamp(min=1)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall).clamp(min=1)\n",
    "    \n",
    "    # Mean of metrics - After handling division by zero, no need for NaN replacement\n",
    "    accuracy = accuracy.mean()\n",
    "    precision = precision.mean()\n",
    "    recall = recall.mean()\n",
    "    f1_score = f1_score.mean()\n",
    "    \n",
    "    return accuracy.item(), precision.item(), recall.item(), f1_score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_injected_layer_early_stopping(model, train_loader, val_loader, num_epochs, criterion, learning_rate, weight_decay, early_stopping_patience=10):\n",
    "\n",
    "    early_stopping_patience = num_epochs // 10\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    max_val_accuracy = 0\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stop:\n",
    "            print(f'Stopping early at epoch {epoch} of {num_epochs} as there has been no improvement in {early_stopping_patience} epochs.')\n",
    "            break\n",
    "\n",
    "        for data in train_loader: \n",
    "            _, embeddings, labels = data\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(embeddings)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Calculate validation metrics per epoch\n",
    "        with torch.no_grad():\n",
    "            overall_epoch_accuracy = 0\n",
    "            overall_val_loss = 0\n",
    "            \n",
    "            for data in val_loader:\n",
    "                _, embeddings, labels = data\n",
    "                embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "                outputs = model(embeddings)\n",
    "                loss = criterion(outputs, labels)\n",
    "                batch_accuracy, batch_precision, batch_recall, batch_f1 = calculate_metrics(outputs,labels)\n",
    "                \n",
    "                overall_val_loss += loss.item()\n",
    "                overall_epoch_accuracy += batch_accuracy\n",
    "            \n",
    "            overall_val_loss /= len(val_loader)\n",
    "            overall_epoch_accuracy /= len(val_loader)\n",
    "\n",
    "            # Early Stopping condition\n",
    "            if overall_epoch_accuracy > max_val_accuracy:\n",
    "                max_val_accuracy = overall_epoch_accuracy\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                \n",
    "                if epochs_no_improve == early_stopping_patience:\n",
    "                    early_stop = True\n",
    "\n",
    "    return model, max_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early at epoch 5 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.01, Num epochs: 10, Criterion: FocalLoss(), Max val accuracy: 0.6739583373069763\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.01, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6583333373069763\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.01, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6677083373069763\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.01, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6666666686534881\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.01, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.68125\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.01, Num epochs: 20, Criterion: FocalLoss(), Max val accuracy: 0.6354166686534881\n",
      "Stopping early at epoch 8 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.01, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6802083373069763\n",
      "Stopping early at epoch 5 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.01, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6729166686534882\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.01, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6697916686534882\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.01, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6802083373069763\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.01, Num epochs: 30, Criterion: FocalLoss(), Max val accuracy: 0.659375\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.01, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6666666686534881\n",
      "Stopping early at epoch 9 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.01, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6947916686534882\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.01, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6770833373069763\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.01, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.69375\n",
      "Stopping early at epoch 8 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.01, Num epochs: 40, Criterion: FocalLoss(), Max val accuracy: 0.6614583373069763\n",
      "Stopping early at epoch 7 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.01, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.665625\n",
      "Stopping early at epoch 13 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.01, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6833333373069763\n",
      "Stopping early at epoch 15 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.01, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.696875\n",
      "Stopping early at epoch 11 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.01, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6927083373069763\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.001, Num epochs: 10, Criterion: FocalLoss(), Max val accuracy: 0.6375\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.001, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6833333373069763\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.001, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6708333373069764\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.001, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6802083373069763\n",
      "Stopping early at epoch 4 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.001, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.675\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.001, Num epochs: 20, Criterion: FocalLoss(), Max val accuracy: 0.6395833373069764\n",
      "Stopping early at epoch 5 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.001, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.68125\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.001, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6770833373069763\n",
      "Stopping early at epoch 6 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.001, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6927083373069763\n",
      "Stopping early at epoch 8 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.001, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6895833373069763\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.001, Num epochs: 30, Criterion: FocalLoss(), Max val accuracy: 0.64375\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.001, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.68125\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.001, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6833333373069763\n",
      "Stopping early at epoch 11 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.001, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.690625\n",
      "Stopping early at epoch 11 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.001, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.690625\n",
      "Stopping early at epoch 9 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.001, Num epochs: 40, Criterion: FocalLoss(), Max val accuracy: 0.6479166686534882\n",
      "Stopping early at epoch 11 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.001, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6822916686534881\n",
      "Stopping early at epoch 15 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.001, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6958333373069763\n",
      "Stopping early at epoch 14 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.001, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6989583373069763\n",
      "Stopping early at epoch 9 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.001, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.684375\n",
      "Stopping early at epoch 6 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 10, Criterion: FocalLoss(), Max val accuracy: 0.6375\n",
      "Stopping early at epoch 4 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.671875\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6645833373069763\n",
      "Stopping early at epoch 4 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6864583373069764\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6916666686534881\n",
      "Stopping early at epoch 5 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 20, Criterion: FocalLoss(), Max val accuracy: 0.6322916686534882\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.675\n",
      "Stopping early at epoch 6 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6927083373069763\n",
      "Stopping early at epoch 6 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6729166686534882\n",
      "Stopping early at epoch 5 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6802083373069763\n",
      "Stopping early at epoch 10 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 30, Criterion: FocalLoss(), Max val accuracy: 0.6572916686534882\n",
      "Stopping early at epoch 10 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6979166686534881\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.675\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6854166686534882\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6854166686534882\n",
      "Stopping early at epoch 5 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 40, Criterion: FocalLoss(), Max val accuracy: 0.6385416686534882\n",
      "Stopping early at epoch 13 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.690625\n",
      "Stopping early at epoch 11 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6979166686534881\n",
      "Stopping early at epoch 7 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6895833373069763\n",
      "Stopping early at epoch 6 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6833333373069763\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decays = [0.01, 0.001, 0.0001]\n",
    "num_epochs = [10, 20, 30, 40]\n",
    "\n",
    "# setup different loss functions for each weight_for_1\n",
    "weight_for_0 = 1\n",
    "weight_for_1 = [1,5,10,20]\n",
    "criterions = [FocalLoss(alpha=0.85, gamma=2.0)]\n",
    "for w in weight_for_1:\n",
    "    weights = torch.tensor([weight_for_0, w])\n",
    "    pos_weights = weights[1]/weights[0]\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weights)\n",
    "    criterions.append(criterion)\n",
    "\n",
    "# setup a grid search for all hyperparameters\n",
    "for weight_decay in weight_decays:\n",
    "    for num_epoch in num_epochs:\n",
    "        for criterion in criterions:\n",
    "            model = AddedGatedResidualLayer(768, len(dataset.pathology_columns))\n",
    "            model, max_val_accuracy = train_injected_layer_early_stopping(model, train_loader, val_loader, num_epoch, criterion, learning_rate, weight_decay)\n",
    "            print(f'Weight decay: {weight_decay}, Num epochs: {num_epoch}, Criterion: {criterion}, Max val accuracy: {max_val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early at epoch 4 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.01, Num epochs: 10, Criterion: FocalLoss(), Max val accuracy: 0.6697916686534882\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.01, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6833333373069763\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.01, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.690625\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.01, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.70625\n",
      "Stopping early at epoch 4 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.01, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7083333373069763\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.01, Num epochs: 20, Criterion: FocalLoss(), Max val accuracy: 0.6635416686534882\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.01, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6927083373069763\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.01, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7010416686534882\n",
      "Stopping early at epoch 5 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.01, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7104166686534882\n",
      "Stopping early at epoch 7 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.01, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.715625\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.01, Num epochs: 30, Criterion: FocalLoss(), Max val accuracy: 0.6666666686534881\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.01, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6875\n",
      "Stopping early at epoch 11 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.01, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7041666686534882\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.01, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6927083373069763\n",
      "Stopping early at epoch 8 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.01, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7083333373069763\n",
      "Stopping early at epoch 6 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.01, Num epochs: 40, Criterion: FocalLoss(), Max val accuracy: 0.6770833373069763\n",
      "Stopping early at epoch 8 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.01, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6947916686534882\n",
      "Stopping early at epoch 15 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.01, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7125\n",
      "Stopping early at epoch 7 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.01, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7135416686534881\n",
      "Stopping early at epoch 9 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.01, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7208333373069763\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.001, Num epochs: 10, Criterion: FocalLoss(), Max val accuracy: 0.634375\n",
      "Stopping early at epoch 5 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.001, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7166666686534882\n",
      "Stopping early at epoch 4 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.001, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7052083373069763\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.001, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.6989583373069763\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.001, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7135416686534881\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.001, Num epochs: 20, Criterion: FocalLoss(), Max val accuracy: 0.6510416686534881\n",
      "Stopping early at epoch 6 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.001, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7114583373069763\n",
      "Stopping early at epoch 7 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.001, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7114583373069763\n",
      "Stopping early at epoch 5 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.001, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.721875\n",
      "Stopping early at epoch 8 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.001, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7125\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.001, Num epochs: 30, Criterion: FocalLoss(), Max val accuracy: 0.6614583373069763\n",
      "Stopping early at epoch 8 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.001, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7052083373069763\n",
      "Stopping early at epoch 6 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.001, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7114583373069763\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.001, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.709375\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.001, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7145833373069763\n",
      "Stopping early at epoch 9 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.001, Num epochs: 40, Criterion: FocalLoss(), Max val accuracy: 0.6791666686534882\n",
      "Stopping early at epoch 15 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.001, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.715625\n",
      "Stopping early at epoch 8 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.001, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7197916686534882\n",
      "Stopping early at epoch 7 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.001, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7135416686534881\n",
      "Stopping early at epoch 6 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.001, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7135416686534881\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 10, Criterion: FocalLoss(), Max val accuracy: 0.6645833373069763\n",
      "Stopping early at epoch 6 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7197916686534882\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7166666686534882\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 10, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7135416686534881\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 20, Criterion: FocalLoss(), Max val accuracy: 0.640625\n",
      "Stopping early at epoch 5 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7166666686534882\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7104166686534882\n",
      "Stopping early at epoch 5 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7072916686534881\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 20, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7145833373069763\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 30, Criterion: FocalLoss(), Max val accuracy: 0.64375\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7114583373069763\n",
      "Stopping early at epoch 9 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7177083373069764\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7052083373069763\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 30, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7114583373069763\n",
      "Stopping early at epoch 10 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 40, Criterion: FocalLoss(), Max val accuracy: 0.6489583373069763\n",
      "Stopping early at epoch 8 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7177083373069764\n",
      "Stopping early at epoch 11 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.721875\n",
      "Stopping early at epoch 9 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7041666686534882\n",
      "Stopping early at epoch 12 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 40, Criterion: BCEWithLogitsLoss(), Max val accuracy: 0.7104166686534882\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decays = [0.01, 0.001, 0.0001]\n",
    "num_epochs = [10, 20, 30, 40]\n",
    "\n",
    "# setup different loss functions for each weight_for_1\n",
    "weight_for_0 = 1\n",
    "weight_for_1 = [1,5,10,20]\n",
    "criterions = [FocalLoss(alpha=0.85, gamma=2.0)]\n",
    "for w in weight_for_1:\n",
    "    weights = torch.tensor([weight_for_0, w])\n",
    "    pos_weights = weights[1]/weights[0]\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weights)\n",
    "    criterions.append(criterion)\n",
    "\n",
    "# setup a grid search for all hyperparameters\n",
    "for weight_decay in weight_decays:\n",
    "    for num_epoch in num_epochs:\n",
    "        for criterion in criterions:\n",
    "            model = StackedAddedGatedResidualLayer(768, len(dataset.pathology_columns))\n",
    "            model, max_val_accuracy = train_injected_layer_early_stopping(model, train_loader, val_loader, num_epoch, criterion, learning_rate, weight_decay)\n",
    "            print(f'Weight decay: {weight_decay}, Num epochs: {num_epoch}, Criterion: {criterion}, Max val accuracy: {max_val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.01, Num epochs: 10, Criterion: 0, Max val accuracy: 0.6864583373069764\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.01, Num epochs: 10, Criterion: 1, Max val accuracy: 0.6895833373069763\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.01, Num epochs: 10, Criterion: 5, Max val accuracy: 0.69375\n",
      "Stopping early at epoch 4 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.01, Num epochs: 10, Criterion: 10, Max val accuracy: 0.696875\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.01, Num epochs: 10, Criterion: 20, Max val accuracy: 0.7052083373069763\n",
      "Stopping early at epoch 6 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.01, Num epochs: 20, Criterion: 0, Max val accuracy: 0.6645833373069763\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.01, Num epochs: 20, Criterion: 1, Max val accuracy: 0.6760416686534881\n",
      "Stopping early at epoch 7 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.01, Num epochs: 20, Criterion: 5, Max val accuracy: 0.7072916686534881\n",
      "Stopping early at epoch 6 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.01, Num epochs: 20, Criterion: 10, Max val accuracy: 0.7114583373069763\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.01, Num epochs: 20, Criterion: 20, Max val accuracy: 0.7072916686534881\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.01, Num epochs: 30, Criterion: 0, Max val accuracy: 0.6822916686534881\n",
      "Stopping early at epoch 8 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.01, Num epochs: 30, Criterion: 1, Max val accuracy: 0.6885416686534882\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.01, Num epochs: 30, Criterion: 5, Max val accuracy: 0.7072916686534881\n",
      "Stopping early at epoch 11 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.01, Num epochs: 30, Criterion: 10, Max val accuracy: 0.7125\n",
      "Stopping early at epoch 12 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.01, Num epochs: 30, Criterion: 20, Max val accuracy: 0.7135416686534881\n",
      "Stopping early at epoch 5 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.01, Num epochs: 40, Criterion: 0, Max val accuracy: 0.6864583373069764\n",
      "Stopping early at epoch 10 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.01, Num epochs: 40, Criterion: 1, Max val accuracy: 0.6927083373069763\n",
      "Stopping early at epoch 8 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.01, Num epochs: 40, Criterion: 5, Max val accuracy: 0.71875\n",
      "Stopping early at epoch 12 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.01, Num epochs: 40, Criterion: 10, Max val accuracy: 0.7125\n",
      "Stopping early at epoch 11 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.01, Num epochs: 40, Criterion: 20, Max val accuracy: 0.7197916686534882\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.001, Num epochs: 10, Criterion: 0, Max val accuracy: 0.659375\n",
      "Stopping early at epoch 5 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.001, Num epochs: 10, Criterion: 1, Max val accuracy: 0.696875\n",
      "Stopping early at epoch 5 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.001, Num epochs: 10, Criterion: 5, Max val accuracy: 0.7229166686534881\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.001, Num epochs: 10, Criterion: 10, Max val accuracy: 0.7052083373069763\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.001, Num epochs: 10, Criterion: 20, Max val accuracy: 0.7\n",
      "Stopping early at epoch 8 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.001, Num epochs: 20, Criterion: 0, Max val accuracy: 0.6614583373069763\n",
      "Stopping early at epoch 10 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.001, Num epochs: 20, Criterion: 1, Max val accuracy: 0.7229166686534881\n",
      "Stopping early at epoch 6 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.001, Num epochs: 20, Criterion: 5, Max val accuracy: 0.7125\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.001, Num epochs: 20, Criterion: 10, Max val accuracy: 0.6989583373069763\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.001, Num epochs: 20, Criterion: 20, Max val accuracy: 0.7052083373069763\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.001, Num epochs: 30, Criterion: 0, Max val accuracy: 0.6302083343267441\n",
      "Stopping early at epoch 6 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.001, Num epochs: 30, Criterion: 1, Max val accuracy: 0.7052083373069763\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.001, Num epochs: 30, Criterion: 5, Max val accuracy: 0.709375\n",
      "Stopping early at epoch 10 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.001, Num epochs: 30, Criterion: 10, Max val accuracy: 0.7145833373069763\n",
      "Stopping early at epoch 9 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.001, Num epochs: 30, Criterion: 20, Max val accuracy: 0.71875\n",
      "Stopping early at epoch 8 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.001, Num epochs: 40, Criterion: 0, Max val accuracy: 0.6666666686534881\n",
      "Stopping early at epoch 7 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.001, Num epochs: 40, Criterion: 1, Max val accuracy: 0.7020833373069764\n",
      "Stopping early at epoch 13 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.001, Num epochs: 40, Criterion: 5, Max val accuracy: 0.7177083373069764\n",
      "Stopping early at epoch 10 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.001, Num epochs: 40, Criterion: 10, Max val accuracy: 0.7197916686534882\n",
      "Stopping early at epoch 9 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.001, Num epochs: 40, Criterion: 20, Max val accuracy: 0.7177083373069764\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 10, Criterion: 0, Max val accuracy: 0.6479166686534882\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 10, Criterion: 1, Max val accuracy: 0.6895833373069763\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 10, Criterion: 5, Max val accuracy: 0.70625\n",
      "Stopping early at epoch 4 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 10, Criterion: 10, Max val accuracy: 0.7010416686534882\n",
      "Stopping early at epoch 5 of 10 as there has been no improvement in 1 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 10, Criterion: 20, Max val accuracy: 0.7010416686534882\n",
      "Stopping early at epoch 6 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 20, Criterion: 0, Max val accuracy: 0.6489583373069763\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 20, Criterion: 1, Max val accuracy: 0.696875\n",
      "Stopping early at epoch 9 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 20, Criterion: 5, Max val accuracy: 0.7083333373069763\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 20, Criterion: 10, Max val accuracy: 0.7020833373069764\n",
      "Stopping early at epoch 6 of 20 as there has been no improvement in 2 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 20, Criterion: 20, Max val accuracy: 0.7166666686534882\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 30, Criterion: 0, Max val accuracy: 0.6489583373069763\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 30, Criterion: 1, Max val accuracy: 0.7114583373069763\n",
      "Stopping early at epoch 16 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 30, Criterion: 5, Max val accuracy: 0.7145833373069763\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 30, Criterion: 10, Max val accuracy: 0.7114583373069763\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 30, Criterion: 20, Max val accuracy: 0.725\n",
      "Stopping early at epoch 8 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 40, Criterion: 0, Max val accuracy: 0.6604166686534881\n",
      "Stopping early at epoch 7 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 40, Criterion: 1, Max val accuracy: 0.7072916686534881\n",
      "Stopping early at epoch 14 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 40, Criterion: 5, Max val accuracy: 0.7145833373069763\n",
      "Stopping early at epoch 8 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 40, Criterion: 10, Max val accuracy: 0.7114583373069763\n",
      "Stopping early at epoch 8 of 40 as there has been no improvement in 4 epochs.\n",
      "Weight decay: 0.0001, Num epochs: 40, Criterion: 20, Max val accuracy: 0.7114583373069763\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decays = [0.01, 0.001, 0.0001]\n",
    "num_epochs = [10, 20, 30, 40]\n",
    "\n",
    "# setup different loss functions for each weight_for_1\n",
    "weight_for_0 = 1\n",
    "weight_for_1 = [0,1,5,10,20]    \n",
    "\n",
    "for weight_decay in weight_decays:\n",
    "    for num_epoch in num_epochs:\n",
    "        for w in weight_for_1:\n",
    "            if w == 0:\n",
    "                criterion = FocalLoss(alpha=0.85, gamma=2.0)  # Assuming FocalLoss is defined elsewhere\n",
    "            else:\n",
    "                weights = torch.tensor([1, w])  # Assuming weight_for_0 is always 1\n",
    "                pos_weights = weights[1] / weights[0]\n",
    "                criterion = nn.BCEWithLogitsLoss(pos_weights)\n",
    "            \n",
    "            model = StackedAddedGatedResidualLayer(768, len(dataset.pathology_columns))\n",
    "            model, max_val_accuracy = train_injected_layer_early_stopping(model, train_loader, val_loader, num_epoch, criterion, learning_rate, weight_decay)\n",
    "            print(f'Weight decay: {weight_decay}, Num epochs: {num_epoch}, Criterion: {w}, Max val accuracy: {max_val_accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "attention_heads = [1, 2, 4]\n",
    "fine_grained_gating = [True, False]\n",
    "\n",
    "# training hyperparameters\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "weight_decays = [0.01, 0.001, 0.0001]\n",
    "num_epochs = [10, 20, 30, 40]\n",
    "\n",
    "# setup different loss functions for each weight_for_1\n",
    "weight_for_0 = 1\n",
    "weight_for_1 = [0,1,5,10,20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 30, Weight for 1: 0\n",
      "Max Val Accuracy: 0.5572916686534881\n",
      "Stopping early at epoch 6 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6677083373069763\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6677083373069763\n",
      "Stopping early at epoch 5 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 40, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6625\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6854166686534882\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 20, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6666666686534881\n",
      "Stopping early at epoch 5 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 40, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6947916686534882\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 30, Weight for 1: 0\n",
      "Max Val Accuracy: 0.66875\n",
      "Stopping early at epoch 7 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 5\n",
      "Max Val Accuracy: 0.678125\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 30, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6729166686534882\n",
      "Stopping early at epoch 6 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 20, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6677083373069763\n",
      "Stopping early at epoch 5 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 40, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6583333373069763\n",
      "Stopping early at epoch 6 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 1\n",
      "Max Val Accuracy: 0.68125\n",
      "Stopping early at epoch 6 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 10\n",
      "Max Val Accuracy: 0.68125\n",
      "Stopping early at epoch 7 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 20, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6760416686534881\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 30, Weight for 1: 5\n",
      "Max Val Accuracy: 0.684375\n",
      "Stopping early at epoch 13 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 40, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6854166686534882\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 10, Weight for 1: 5\n",
      "Max Val Accuracy: 0.65625\n",
      "Stopping early at epoch 13 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 1\n",
      "Max Val Accuracy: 0.69375\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 30, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6385416686534882\n",
      "Stopping early at epoch 6 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 20, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6677083373069763\n",
      "Stopping early at epoch 4 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 5\n",
      "Max Val Accuracy: 0.678125\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 0\n",
      "Max Val Accuracy: 0.5041666686534881\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 10, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6583333373069763\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 20, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6885416686534882\n",
      "Stopping early at epoch 7 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 40, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6614583373069763\n",
      "Stopping early at epoch 5 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 20, Weight for 1: 10\n",
      "Max Val Accuracy: 0.690625\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 10, Weight for 1: 20\n",
      "Max Val Accuracy: 0.675\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 30, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6677083373069763\n",
      "Stopping early at epoch 8 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 40, Weight for 1: 20\n",
      "Max Val Accuracy: 0.68125\n",
      "Stopping early at epoch 6 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 0\n",
      "Max Val Accuracy: 0.65\n",
      "Stopping early at epoch 14 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 40, Weight for 1: 5\n",
      "Max Val Accuracy: 0.7072916686534881\n",
      "Stopping early at epoch 4 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 10, Weight for 1: 5\n",
      "Max Val Accuracy: 0.675\n",
      "Stopping early at epoch 5 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 20, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6854166686534882\n",
      "Stopping early at epoch 10 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 40, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6885416686534882\n",
      "Stopping early at epoch 12 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 20, Weight for 1: 5\n",
      "Max Val Accuracy: 0.696875\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 10, Weight for 1: 1\n",
      "Max Val Accuracy: 0.66875\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 30, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6770833373069763\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6625\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 20, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6625\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 20, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6572916686534882\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6541666686534882\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 20, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6708333373069764\n",
      "Stopping early at epoch 10 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 40, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6864583373069764\n",
      "Stopping early at epoch 4 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 0\n",
      "Max Val Accuracy: 0.64375\n",
      "Stopping early at epoch 18 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6802083373069763\n",
      "Stopping early at epoch 7 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 40, Weight for 1: 5\n",
      "Max Val Accuracy: 0.675\n",
      "Stopping early at epoch 6 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 1\n",
      "Max Val Accuracy: 0.68125\n",
      "Stopping early at epoch 6 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 10, Weight for 1: 0\n",
      "Max Val Accuracy: 0.671875\n",
      "Stopping early at epoch 8 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 40, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6760416686534881\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 20, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6677083373069763\n",
      "Stopping early at epoch 6 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6729166686534882\n",
      "Stopping early at epoch 6 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 40, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6625\n",
      "Stopping early at epoch 11 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 30, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6885416686534882\n",
      "Stopping early at epoch 8 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 30, Weight for 1: 0\n",
      "Max Val Accuracy: 0.665625\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 20, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6739583373069763\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 20, Weight for 1: 10\n",
      "Max Val Accuracy: 0.675\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6770833373069763\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 20, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6916666686534881\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 10, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6729166686534882\n",
      "Stopping early at epoch 7 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 20, Weight for 1: 20\n",
      "Max Val Accuracy: 0.690625\n",
      "Stopping early at epoch 6 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6791666686534882\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 20\n",
      "Max Val Accuracy: 0.659375\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 30, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6885416686534882\n",
      "Stopping early at epoch 6 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 20, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6833333373069763\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 30, Weight for 1: 20\n",
      "Max Val Accuracy: 0.675\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 20, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6979166686534881\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 30, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6583333373069763\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 20, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6697916686534882\n",
      "Stopping early at epoch 10 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6875\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6625\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 10, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6864583373069764\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 20, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6572916686534882\n",
      "Stopping early at epoch 7 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 20, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6729166686534882\n",
      "Stopping early at epoch 6 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 20, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6916666686534881\n",
      "Stopping early at epoch 7 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 20, Weight for 1: 5\n",
      "Max Val Accuracy: 0.696875\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6375\n",
      "Stopping early at epoch 5 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 40, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6572916686534882\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 20, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6916666686534881\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 30, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6625\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 10, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6895833373069763\n",
      "Stopping early at epoch 9 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 20, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6875\n",
      "Stopping early at epoch 13 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 10\n",
      "Max Val Accuracy: 0.69375\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 30, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6833333373069763\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 30, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6625\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6625\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 20, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6520833373069763\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 10, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6697916686534882\n",
      "Stopping early at epoch 6 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6833333373069763\n",
      "Stopping early at epoch 8 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6791666686534882\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 20, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6875\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 20, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6802083373069763\n",
      "Stopping early at epoch 14 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6489583373069763\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 30, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6833333373069763\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 30, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6510416686534881\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 10, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6739583373069763\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 10, Weight for 1: 0\n",
      "Max Val Accuracy: 0.559375\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6958333373069763\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 10, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6572916686534882\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 30, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6645833373069763\n",
      "Stopping early at epoch 5 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 10, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6729166686534882\n",
      "Stopping early at epoch 6 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 30, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6822916686534881\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 20, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6822916686534881\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 20, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6916666686534881\n",
      "Stopping early at epoch 6 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 30, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6739583373069763\n",
      "Stopping early at epoch 8 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 30, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6895833373069763\n",
      "Stopping early at epoch 16 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6947916686534882\n",
      "Stopping early at epoch 5 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 40, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6635416686534882\n",
      "Stopping early at epoch 8 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6510416686534881\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6677083373069763\n",
      "Stopping early at epoch 4 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 10, Weight for 1: 0\n",
      "Max Val Accuracy: 0.542708334326744\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6572916686534882\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 10, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6729166686534882\n",
      "Stopping early at epoch 5 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6625\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 10, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6739583373069763\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 10, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6572916686534882\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 10, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6645833373069763\n",
      "Stopping early at epoch 9 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6885416686534882\n",
      "Stopping early at epoch 6 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 20, Weight for 1: 0\n",
      "Max Val Accuracy: 0.4395833373069763\n",
      "Stopping early at epoch 4 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 10, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6729166686534882\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6739583343267441\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 10, Weight for 1: 20\n",
      "Max Val Accuracy: 0.675\n",
      "Stopping early at epoch 9 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 30, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6895833373069763\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 30, Weight for 1: 0\n",
      "Max Val Accuracy: 0.5177083373069763\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 20, Weight for 1: 1\n",
      "Max Val Accuracy: 0.675\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 30, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6677083373069763\n",
      "Stopping early at epoch 6 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 30, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6447916686534881\n",
      "Stopping early at epoch 8 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6677083373069763\n",
      "Stopping early at epoch 10 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 40, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6760416686534881\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 20, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6895833373069763\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 10, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6833333373069763\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 10, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6885416686534882\n",
      "Stopping early at epoch 7 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 40, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6375\n",
      "Stopping early at epoch 10 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6927083373069763\n",
      "Stopping early at epoch 7 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 40, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6625\n",
      "Stopping early at epoch 6 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 40, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6833333373069763\n",
      "Stopping early at epoch 11 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 40, Weight for 1: 0\n",
      "Max Val Accuracy: 0.5375\n",
      "Stopping early at epoch 6 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 20\n",
      "Max Val Accuracy: 0.690625\n",
      "Stopping early at epoch 7 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 40, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6895833373069763\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 10, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6677083373069763\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 20, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6572916686534882\n",
      "Stopping early at epoch 6 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 20, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6645833373069763\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 20, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6770833373069763\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 10, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6770833373069763\n",
      "Stopping early at epoch 6 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6833333373069763\n",
      "Stopping early at epoch 11 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 20\n",
      "Max Val Accuracy: 0.671875\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 30, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6822916686534881\n",
      "Stopping early at epoch 8 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 20, Weight for 1: 5\n",
      "Max Val Accuracy: 0.696875\n",
      "Stopping early at epoch 9 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 40, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6854166686534882\n",
      "Stopping early at epoch 5 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 20, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6895833373069763\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 30, Weight for 1: 1\n",
      "Max Val Accuracy: 0.684375\n",
      "Stopping early at epoch 6 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 30, Weight for 1: 1\n",
      "Max Val Accuracy: 0.7\n",
      "Stopping early at epoch 6 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 20, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6479166686534882\n",
      "Stopping early at epoch 5 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 20, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6802083373069763\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 10, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6666666686534881\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 10, Weight for 1: 5\n",
      "Max Val Accuracy: 0.671875\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6489583373069763\n",
      "Stopping early at epoch 10 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6739583373069763\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6760416686534881\n",
      "Stopping early at epoch 11 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 40, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6791666686534882\n",
      "Stopping early at epoch 3 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 20, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6791666686534882\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 30, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6677083373069763\n",
      "Stopping early at epoch 17 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 40, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6947916686534882\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 30, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6666666686534881\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 10, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6677083373069763\n",
      "Stopping early at epoch 6 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 30, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6677083373069763\n",
      "Stopping early at epoch 6 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 30, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6708333373069764\n",
      "Stopping early at epoch 5 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 40, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6739583373069763\n",
      "Stopping early at epoch 10 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6760416686534881\n",
      "Stopping early at epoch 7 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 0\n",
      "Max Val Accuracy: 0.621875\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 20, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6604166686534881\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6614583373069763\n",
      "Stopping early at epoch 5 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 40, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6864583373069764\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 10, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6739583373069763\n",
      "Stopping early at epoch 9 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 40, Weight for 1: 1\n",
      "Max Val Accuracy: 0.675\n",
      "Stopping early at epoch 7 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 40, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6572916686534882\n",
      "Stopping early at epoch 6 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 0\n",
      "Max Val Accuracy: 0.5666666686534881\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 30, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6729166686534882\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 10, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6854166686534882\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.01, Num Epochs: 10, Weight for 1: 0\n",
      "Max Val Accuracy: 0.6458333373069763\n",
      "Stopping early at epoch 6 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6697916686534882\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 20, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6520833373069763\n",
      "Stopping early at epoch 4 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6833333373069763\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 10, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6895833373069763\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.01, Num Epochs: 10, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6833333373069763\n",
      "Stopping early at epoch 11 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6770833373069763\n",
      "Stopping early at epoch 13 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 1\n",
      "Max Val Accuracy: 0.6927083373069763\n",
      "Stopping early at epoch 3 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 0\n",
      "Max Val Accuracy: 0.5020833343267441\n",
      "Stopping early at epoch 6 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: False, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 40, Weight for 1: 5\n",
      "Max Val Accuracy: 0.68125\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.001, Weight Decay: 0.001, Num Epochs: 30, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6875\n",
      "Stopping early at epoch 2 of 10 as there has been no improvement in 1 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 10, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6572916686534882\n",
      "Stopping early at epoch 4 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6666666686534881\n",
      "Stopping early at epoch 10 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 40, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6708333373069764\n",
      "Stopping early at epoch 9 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: False, Learning Rate: 0.01, Weight Decay: 0.001, Num Epochs: 40, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6927083373069763\n",
      "Stopping early at epoch 5 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 40, Weight for 1: 5\n",
      "Max Val Accuracy: 0.678125\n",
      "Stopping early at epoch 5 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 1, Fine Grained Gating: False, Learning Rate: 0.0001, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6677083373069763\n",
      "Stopping early at epoch 7 of 30 as there has been no improvement in 3 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.01, Weight Decay: 0.0001, Num Epochs: 30, Weight for 1: 0\n",
      "Max Val Accuracy: 0.5520833373069763\n",
      "Stopping early at epoch 6 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.01, Num Epochs: 20, Weight for 1: 5\n",
      "Max Val Accuracy: 0.6677083373069763\n",
      "Stopping early at epoch 17 of 40 as there has been no improvement in 4 epochs.\n",
      "Attention Heads: 2, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 40, Weight for 1: 20\n",
      "Max Val Accuracy: 0.6770833373069763\n",
      "Stopping early at epoch 4 of 20 as there has been no improvement in 2 epochs.\n",
      "Attention Heads: 4, Fine Grained Gating: True, Learning Rate: 0.0001, Weight Decay: 0.001, Num Epochs: 20, Weight for 1: 10\n",
      "Max Val Accuracy: 0.6677083373069763\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters sets generation\n",
    "hyperparam_sets = itertools.product(attention_heads, fine_grained_gating, learning_rates, weight_decays, num_epochs, weight_for_1)\n",
    "\n",
    "# Randomly select combinations to try\n",
    "num_trials = 200  # Number of trials to conduct\n",
    "selected_hyperparams = random.sample(list(hyperparam_sets), num_trials)\n",
    "\n",
    "for head, gate, lr, wd, ep, w in selected_hyperparams:\n",
    "    if w == 0:\n",
    "        criterion = FocalLoss(alpha=0.85, gamma=2.0)  # Assuming FocalLoss is defined elsewhere\n",
    "    else:\n",
    "        weights = torch.tensor([1, w])  # Assuming weight_for_0 is always 1\n",
    "        pos_weights = weights[1] / weights[0]\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weights)\n",
    "        \n",
    "    model = AddedGatedResidualLayer(768, len(dataset.pathology_columns), attention_heads=head, fine_grained_gating=gate)\n",
    "    model, max_val_accuracy = train_injected_layer_early_stopping(model, train_loader, val_loader, ep, criterion, lr, wd)\n",
    "    print(f'Attention Heads: {head}, Fine Grained Gating: {gate}, Learning Rate: {lr}, Weight Decay: {wd}, Num Epochs: {ep}, Weight for 1: {w}')\n",
    "    print(f'Max Val Accuracy: {max_val_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Validation Accuracy: 0.746875, Line: \"Validation Accuracy: 0.746875\\n\",\n",
      "Top 2 Validation Accuracy: 0.746875, Line: \"Validation Accuracy: 0.746875\\n\",\n",
      "Top 3 Validation Accuracy: 0.746875, Line: \"Validation Accuracy: 0.746875\\n\",\n",
      "Top 4 Validation Accuracy: 0.7458333373069763, Line: \"Validation Accuracy: 0.7458333373069763\\n\",\n",
      "Top 5 Validation Accuracy: 0.7447916686534881, Line: \"Validation Accuracy: 0.7447916686534881\\n\",\n",
      "Top 6 Validation Accuracy: 0.7447916686534881, Line: \"Validation Accuracy: 0.7447916686534881\\n\",\n",
      "Top 7 Validation Accuracy: 0.7447916686534881, Line: \"Validation Accuracy: 0.7447916686534881\\n\",\n",
      "Top 8 Validation Accuracy: 0.7447916686534881, Line: \"Validation Accuracy: 0.7447916686534881\\n\",\n",
      "Top 9 Validation Accuracy: 0.74375, Line: \"Validation Accuracy: 0.74375\\n\",\n",
      "Top 10 Validation Accuracy: 0.74375, Line: \"Validation Accuracy: 0.74375\\n\",\n"
     ]
    }
   ],
   "source": [
    "# grid search analysis:\n",
    "with open('/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/fine_tuning/inject_layer_grid_search', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    max_val_acc = 0\n",
    "    line_num = 0\n",
    "    top_accuracies = [(0.0, '')] * 10\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"Validation Accuracy\" in line:\n",
    "            accuracy = float(line.split(\":\")[1].split(\"\\\\n\")[0].strip())\n",
    "            # val_acc = float(line.split(\" \")[1])\n",
    "            for i, (top_accuracy, _) in enumerate(top_accuracies):\n",
    "                if accuracy > top_accuracy:\n",
    "                    # Shift down the list and insert the new accuracy and its corresponding line\n",
    "                    top_accuracies.insert(i, (accuracy, line))\n",
    "                    top_accuracies.pop()\n",
    "                    break\n",
    "\n",
    "for i, (accuracy, line) in enumerate(top_accuracies, start=1):\n",
    "    print(f\"Top {i} Validation Accuracy: {accuracy}, Line: {line.strip()}\")  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.794832655787468, Test Accuracy: 0.715625, Test Precision: 0.11927083767950535, Test Recall: 0.0776810523122549, Test F1: 0.08618348687887192\n"
     ]
    }
   ],
   "source": [
    "# calculate test metrics\n",
    "with torch.no_grad():\n",
    "    overall_test_accuracy = 0\n",
    "    overall_test_loss = 0\n",
    "    overall_test_precision = 0\n",
    "    overall_test_recall = 0\n",
    "    overall_test_f1 = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        image_id, embeddings, labels = data\n",
    "        embeddings, labels = embeddings.to(device), labels.to(device)  # Move data to the same device as model\n",
    "        outputs = model(embeddings)\n",
    "        predictions = torch.sigmoid(outputs)  \n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        batch_accuracy, batch_precision, batch_recall, batch_f1 = calculate_metrics(outputs,labels)\n",
    "        overall_test_loss += loss.item()\n",
    "        overall_test_accuracy += batch_accuracy\n",
    "        overall_test_precision += batch_precision\n",
    "        overall_test_recall += batch_recall\n",
    "        overall_test_f1 += batch_f1\n",
    "\n",
    "\n",
    "    overall_test_loss /= len(test_loader)\n",
    "    overall_test_accuracy /= len(test_loader)\n",
    "    overall_test_precision /= len(test_loader)\n",
    "    overall_test_recall /= len(test_loader)\n",
    "    overall_test_f1 /= len(test_loader)\n",
    "    print(f'Test Loss: {overall_test_loss}, Test Accuracy: {overall_test_accuracy}, Test Precision: {overall_test_precision}, Test Recall: {overall_test_recall}, Test F1: {overall_test_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the gated residual layer of the model\n",
    "# path = Path('/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/fine_tuning/saved_layers')\n",
    "# torch.save(model.gated_residual_layer.state_dict(), path/'gated_residual_layer_70.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAVEYARD - OLD TRAINING CODE\n",
    "def train_injected_layer(model, train_loader, val_loader, num_epochs, criterion, learning_rate, weight_decay): \n",
    "\n",
    "    model = model.to(device) # Step 2: Move the model to the selected device\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    max_val_accuracy = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "        for data in train_loader: \n",
    "            _, embeddings, labels = data\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device) # Move data to the same device as model\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(embeddings)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # calculate validation metrics per epoch\n",
    "        with torch.no_grad():\n",
    "            overall_epoch_accuracy = 0\n",
    "            overall_val_loss = 0\n",
    "            \n",
    "            for data in val_loader:\n",
    "                _,embeddings, labels = data\n",
    "                embeddings, labels = embeddings.to(device), labels.to(device)  # Move data to the same device as model\n",
    "                outputs = model(embeddings)\n",
    "                loss = criterion(outputs, labels)\n",
    "                batch_accuracy, batch_precision, batch_recall, batch_f1 = calculate_metrics(outputs,labels)\n",
    "                overall_val_loss += loss.item()\n",
    "                overall_epoch_accuracy += batch_accuracy\n",
    "            \n",
    "            overall_val_loss /= len(val_loader)\n",
    "            overall_epoch_accuracy /= len(val_loader)\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {overall_val_loss}, Validation Accuracy: {overall_epoch_accuracy}')\n",
    "\n",
    "            avg_val_accuracy += overall_epoch_accuracy\n",
    "\n",
    "            if overall_epoch_accuracy > max_val_accuracy:\n",
    "                max_val_accuracy = overall_epoch_accuracy\n",
    "            \n",
    "    return model, max_val_accuracy\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tuning_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
