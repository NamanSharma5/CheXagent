{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['HF_HOME'] = '/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/.cache' ## THIS HAS TO BE BEFORE YOU IMPORT TRANSFORMERS\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:02<00:00,  2.41it/s]\n"
     ]
    }
   ],
   "source": [
    "def setup_model() -> tuple:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.float16\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(\"StanfordAIMI/CheXagent-8b\", trust_remote_code=True)\n",
    "    generation_config = GenerationConfig.from_pretrained(\"StanfordAIMI/CheXagent-8b\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"StanfordAIMI/CheXagent-8b\", torch_dtype=dtype, trust_remote_code=True\n",
    "    ).to(device)\n",
    "\n",
    "    return processor, model, device, dtype, generation_config\n",
    "\n",
    "processor, model, device, dtype, generation_config = setup_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['vision_model', 'qformer', 'language_projection', 'language_model'])\n",
      "MistralRMSNorm()\n",
      "Linear(in_features=4096, out_features=32000, bias=False)\n"
     ]
    }
   ],
   "source": [
    "# print(model.__dict__.keys())\n",
    "print(model.__dict__[\"_modules\"].keys())\n",
    "print(model.language_model.model.norm)\n",
    "print(model.language_model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embeddings_outputs = []\n",
    "\n",
    "def patch_embedding_hook(module, input, output):\n",
    "    \"\"\"Function to be called by the hook for the patch embeddings layer.\"\"\"\n",
    "    print(\"\\n\\nPatch Embedding Hook\")\n",
    "    patch_embeddings_outputs.append(output.cpu().detach())  # Assuming you want to move data to CPU for analysis\n",
    "\n",
    "post_layer_norm_outputs = []\n",
    "\n",
    "def post_layer_norm_hook(module, input, output):\n",
    "    \"\"\"Function to be called by the hook for the post layer norm layer.\"\"\"\n",
    "    print(\"\\nPost Layer Norm Hook\")\n",
    "    print(\"output shape: \", output.shape)\n",
    "    post_layer_norm_outputs.append(output.cpu().detach())  # Assuming you want to move data to CPU for analysis\n",
    "\n",
    "q_former_outputs = []\n",
    "language_projection_outputs = []\n",
    "\n",
    "def language_projection_hook(module, input, output):\n",
    "    \"\"\"Function to be called by the hook for the language projection layer.\"\"\"\n",
    "    print(\"\\nLanguage Projection Hook\")\n",
    "    print(\"Input shape: \", input[0].shape)\n",
    "    q_former_outputs.append(input[0].cpu().detach())  # Assuming you want to move data to CPU for analysis\n",
    "    print(\"Output shape: \", output.shape)\n",
    "    language_projection_outputs.append(output.cpu().detach())  # Assuming you want to move data to CPU for analysis\n",
    "\n",
    "mistral_model_outputs = []\n",
    "\n",
    "def mistral_model_output_hook(module, input, output):\n",
    "    \"\"\"Function to be called by the hook for the mistral norm layer.\"\"\"\n",
    "    print(\"\\nMistral Model Output Hook\")\n",
    "    print(\"Output shape: \", output.shape)\n",
    "    mistral_model_outputs.append(output.cpu().detach())  # Assuming you want to move data to CPU for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample_image_0 \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/vol/biodata/data/chest_xray/mimic-cxr-jpg/files/p10/p10002428/s59891001/df59ff50-56f1fd83-f3664b8b-8e2c67dd-d5f0d039.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m sample_image_1\u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/vol/biodata/data/chest_xray/VinDr-CXR/1.0.0_png_512/raw/test/bfd1974dc9778aadb407a11b57ab748f.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m sample_image_2 \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/vol/biodata/data/chest_xray/VinDr-CXR/1.0.0_png_512/raw/test/39949d5d2d8855e391b87d8a932b5aaf.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "sample_image_0 = Path('/vol/biodata/data/chest_xray/mimic-cxr-jpg/files/p10/p10002428/s59891001/df59ff50-56f1fd83-f3664b8b-8e2c67dd-d5f0d039.jpg')\n",
    "sample_image_1= Path('/vol/biodata/data/chest_xray/VinDr-CXR/1.0.0_png_512/raw/test/bfd1974dc9778aadb407a11b57ab748f.png')\n",
    "sample_image_2 = Path('/vol/biodata/data/chest_xray/VinDr-CXR/1.0.0_png_512/raw/test/39949d5d2d8855e391b87d8a932b5aaf.png')\n",
    "sample_images = [sample_image_0, sample_image_1, sample_image_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m     26\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescribe the findings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m \u001b[43msample_images\u001b[49m:\n\u001b[1;32m     28\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m     response \u001b[38;5;241m=\u001b[39m generate_with_forward_hooks(image, prompt, processor, model, device, dtype, generation_config)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_images' is not defined"
     ]
    }
   ],
   "source": [
    "# images = [Image.open(sample_image_0).convert(\"RGB\"), Image.open(sample_image).convert(\"RGB\"), Image.open(sample_image_2).convert(\"RGB\")]\n",
    "\n",
    "def generate_with_forward_hooks(images, prompt, processor, model, device, dtype, generation_config):\n",
    "    \n",
    "    # register hooks\n",
    "    patch_embeddings = model.vision_model.embeddings.patch_embedding.register_forward_hook(patch_embedding_hook)\n",
    "    post_layer_norm = model.vision_model.post_layernorm.register_forward_hook(post_layer_norm_hook)\n",
    "    language_projection = model.language_projection.register_forward_hook(language_projection_hook)\n",
    "    mistral_model = model.language_model.model.norm.register_forward_hook(mistral_model_output_hook)\n",
    "\n",
    "    # complete a forward pass \n",
    "    inputs = processor(\n",
    "        images=images, text=f\" USER: <s>{prompt} ASSISTANT: <s>\", return_tensors=\"pt\"\n",
    "    ).to(device=device, dtype=dtype)\n",
    "    output = model.generate(**inputs, generation_config=generation_config)[0]\n",
    "    response = processor.tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "    # remove hooks\n",
    "    patch_embeddings.remove()\n",
    "    post_layer_norm.remove()\n",
    "    language_projection.remove()\n",
    "    mistral_model.remove()\n",
    "\n",
    "    return response\n",
    "\n",
    "prompt = \"Describe the findings\"\n",
    "for image in sample_images:\n",
    "    image = Image.open(image).convert(\"RGB\")\n",
    "    response = generate_with_forward_hooks(image, prompt, processor, model, device, dtype, generation_config)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "shape of each q_former output: torch.Size([1, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "print(len(q_former_outputs))\n",
    "print(f\"shape of each q_former output: {q_former_outputs[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of flattened q_former output: torch.Size([98304])\n"
     ]
    }
   ],
   "source": [
    "q_former_outputs_flattened = [output.flatten() for output in q_former_outputs]\n",
    "print(f\"shape of flattened q_former output: {q_former_outputs_flattened[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of q_former outputs numpy array: (3, 98304)\n"
     ]
    }
   ],
   "source": [
    "# construct a 2D numpy array from the flattened q_former outputs\n",
    "q_former_outputs_np = np.array(q_former_outputs_flattened)\n",
    "print(f\"shape of q_former outputs numpy array: {q_former_outputs_np.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "q_former_outputs_pca = pca.fit_transform(q_former_outputs_np)\n",
    "print(q_former_outputs_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the PCA of the q_former outputs\n",
    "plt.scatter(q_former_outputs_pca[:, 0], q_former_outputs_pca[:, 1])\n",
    "plt.title(\"PCA of q_former outputs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs are different\n",
      "4\n",
      "torch.Size([1, 1408, 32, 32])\n",
      "patch_embeddings_flattened.shape=torch.Size([1, 1408, 1024])\n",
      "(1024, 100)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if all(torch.equal(patch_embeddings_outputs[0], output) for output in patch_embeddings_outputs):\n",
    "    print(f\"All {len(patch_embeddings_outputs)} outputs are the same\")\n",
    "    patch_embeddings_outputs = patch_embeddings_outputs[:1]\n",
    "else:\n",
    "    print(f\"Outputs are different\")\n",
    "    print(len(patch_embeddings_outputs))\n",
    "print(patch_embeddings_outputs[0].shape) # the extra dimension supports multiple input images\n",
    "\n",
    "# flatten the patch embeddings from (batch_size, embedding_dim, feature map width, feature map height) to (batch_size,feature map height * feature map width ,embedding_dim)\n",
    "patch_embeddings_flattened = patch_embeddings_outputs[0].reshape(patch_embeddings_outputs[0].shape[0],patch_embeddings_outputs[0].shape[1],-1)\n",
    "print(f\"{patch_embeddings_flattened.shape=}\")\n",
    "\n",
    "# apply PCA to reduce the dimensionality of the patch embeddings\n",
    "pca = PCA(n_components=100)\n",
    "patch_embeddings_pca = pca.fit_transform(patch_embeddings_flattened[0].T)\n",
    "print(patch_embeddings_pca.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.2358694 7.2385015 7.243475  ... 7.2517977 7.251814  7.2522326]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "patch_embeddings_flattened = patch_embeddings_outputs[0].view(-1, patch_embeddings_outputs[0].shape[1])\n",
    "# normalize the patch embeddings so that they are in the range [0,1]\n",
    "patch_embeddings_normalized = (patch_embeddings_flattened - patch_embeddings_flattened.min()) / (patch_embeddings_flattened.max() - patch_embeddings_flattened.min())\n",
    "# calculate the entropy of each patch embedding\n",
    "patch_embeddings_entropy = entropy(patch_embeddings_normalized.T)\n",
    "# sort the entropy values and print the sorted values not the indices\n",
    "print(np.sort(patch_embeddings_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs are different\n",
      "2\n",
      "torch.Size([1, 1025, 1408])\n",
      "torch.Size([1, 1408])\n"
     ]
    }
   ],
   "source": [
    "if all(torch.equal(post_layer_norm_outputs[0], output) for output in post_layer_norm_outputs):\n",
    "    print(f\"All {len(post_layer_norm_outputs)} outputs are the same\")\n",
    "    post_layer_norm_outputs = post_layer_norm_outputs[:1]\n",
    "else:\n",
    "    print(f\"Outputs are different\")\n",
    "    print(len(post_layer_norm_outputs))\n",
    "print(post_layer_norm_outputs[0].shape)\n",
    "print(post_layer_norm_outputs[1].shape)\n",
    "\n",
    "# compare all 1025 elements of post_layer_norm_outputs[0] with post_layer_norm_outputs[1]\n",
    "for i in range(1025):\n",
    "    if torch.equal(post_layer_norm_outputs[0][0][i], post_layer_norm_outputs[1][0]):\n",
    "        print(f\"Element {i} is same\")\n",
    "        print(post_layer_norm_outputs[0][0][i])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 1 outputs are the same\n",
      "torch.Size([1, 128, 4096])\n"
     ]
    }
   ],
   "source": [
    "if all(torch.equal(language_projection_outputs[0], output) for output in language_projection_outputs):\n",
    "    print(f\"All {len(language_projection_outputs)} outputs are the same\")\n",
    "    language_projection_outputs = language_projection_outputs[:1]\n",
    "else:\n",
    "    print(f\"Outputs are different\")\n",
    "    print(len(language_projection_outputs))\n",
    "\n",
    "print(language_projection_outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "perplexity must be less than n_samples",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      3\u001b[0m language_projection_outputs_flattened \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(language_projection_outputs[\u001b[38;5;241m0\u001b[39m], start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m language_projection_outputs_tsne \u001b[38;5;241m=\u001b[39m \u001b[43mtsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage_projection_outputs_flattened\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(language_projection_outputs_tsne\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/.venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 273\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    279\u001b[0m         )\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/.venv/lib/python3.10/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/.venv/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1125\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;66;03m# TSNE.metric is not validated yet\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m )\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m \n\u001b[1;32m   1107\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;124;03m        Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1125\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/.venv/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:836\u001b[0m, in \u001b[0;36mTSNE._check_params_vs_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperplexity \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 836\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity must be less than n_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: perplexity must be less than n_samples"
     ]
    }
   ],
   "source": [
    "# apply T-SNE to language projection outputs\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "language_projection_outputs_flattened = torch.flatten(language_projection_outputs[0], start_dim=1)\n",
    "language_projection_outputs_tsne = tsne.fit_transform(language_projection_outputs_flattened)\n",
    "print(language_projection_outputs_tsne.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "Outputs are different\n",
      "torch.Size([5, 144, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(len(mistral_model_outputs))\n",
    "if all(torch.equal(mistral_model_outputs[0], output) for output in mistral_model_outputs):\n",
    "    print(f\"All {len(mistral_model_outputs)} outputs are the same\")\n",
    "    mistral_model_outputs = mistral_model_outputs[:1]\n",
    "else:\n",
    "    print(f\"Outputs are different\")\n",
    "print(mistral_model_outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pickle to save the outputs\n",
    "# with open(\"patch_embeddings_outputs.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(patch_embeddings_outputs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hi-ml-up-to-date-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
