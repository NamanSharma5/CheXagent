{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['HF_HOME'] = '/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/.cache' ## THIS HAS TO BE BEFORE YOU IMPORT TRANSFORMERS\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [01:41<00:00, 14.56s/it]\n"
     ]
    }
   ],
   "source": [
    "def setup_model() -> tuple:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.float16\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(\"StanfordAIMI/CheXagent-8b\", trust_remote_code=True)\n",
    "    generation_config = GenerationConfig.from_pretrained(\"StanfordAIMI/CheXagent-8b\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"StanfordAIMI/CheXagent-8b\", torch_dtype=dtype, trust_remote_code=True\n",
    "    ).to(device)\n",
    "\n",
    "    return processor, model, device, dtype, generation_config\n",
    "\n",
    "processor, model, device, dtype, generation_config = setup_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embeddings_outputs = []\n",
    "post_layer_norm_outputs = []\n",
    "q_former_outputs = []\n",
    "language_projection_outputs = []\n",
    "mistral_model_outputs = []\n",
    "\n",
    "patch_embeddings_output = None\n",
    "post_layer_norm_output = None\n",
    "q_former_output = None\n",
    "language_projection_output = None\n",
    "mistral_model_output = None\n",
    "\n",
    "def patch_embedding_hook(module, input, output):\n",
    "    \"\"\"Function to be called by the hook for the patch embeddings layer.\"\"\"\n",
    "    output = output.cpu().detach()  # Assuming you want to move data to CPU for analysis\n",
    "    \n",
    "    global patch_embeddings_output \n",
    "    patch_embeddings_output = output\n",
    "    \n",
    "    # patch_embeddings_outputs.append(output)  \n",
    "\n",
    "def post_layer_norm_hook(module, input, output):\n",
    "    \"\"\"Function to be called by the hook for the post layer norm layer.\"\"\"\n",
    "    output = output.cpu().detach()  # Assuming you want to move data to CPU for analysis\n",
    "    \n",
    "    global post_layer_norm_output\n",
    "    post_layer_norm_output = output\n",
    "    \n",
    "    # post_layer_norm_outputs.append(output) \n",
    "\n",
    "def language_projection_hook(module, input, output):\n",
    "    \"\"\"Function to be called by the hook for the language projection layer.\"\"\"\n",
    "    input = input[0].cpu().detach()  # Assuming you want to move data to CPU for analysis\n",
    "    output = output.cpu().detach()  # Assuming you want to move data to CPU for analysis\n",
    "    \n",
    "    global q_former_output\n",
    "    q_former_output = input\n",
    "\n",
    "    global language_projection_output\n",
    "    language_projection_output = output\n",
    "\n",
    "    # q_former_outputs.append(input)\n",
    "    # language_projection_outputs.append(output)\n",
    "\n",
    "def mistral_model_output_hook(module, input, output):\n",
    "    \"\"\"Function to be called by the hook for the mistral norm layer.\"\"\"\n",
    "    output = output.cpu().detach()  # Assuming you want to move data to CPU for analysis\n",
    "    \n",
    "    global mistral_model_output\n",
    "    mistral_model_output = output\n",
    "\n",
    "    # mistral_model_outputs.append(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "def generate_with_forward_hooks(images, prompt, processor, model, device, dtype, generation_config):\n",
    "    image_id = images\n",
    "    images = Image.open(image_id).convert(\"RGB\")\n",
    "\n",
    "    # convert image_id to a string\n",
    "    if isinstance(image_id, Path):\n",
    "        image_id_string = str(image_id).split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    # register hooks\n",
    "    patch_embeddings = model.vision_model.embeddings.patch_embedding.register_forward_hook(patch_embedding_hook)\n",
    "    post_layer_norm = model.vision_model.post_layernorm.register_forward_hook(post_layer_norm_hook)\n",
    "    language_projection = model.language_projection.register_forward_hook(language_projection_hook)\n",
    "    # mistral_model = model.language_model.model.norm.register_forward_hook(mistral_model_output_hook)\n",
    "\n",
    "    # complete a forward pass \n",
    "    inputs = processor(\n",
    "        images=images, text=f\" USER: <s>{prompt} ASSISTANT: <s>\", return_tensors=\"pt\"\n",
    "    ).to(device=device, dtype=dtype)\n",
    "    output = model.generate(**inputs, generation_config=generation_config)[0]\n",
    "    response = processor.tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "    global embeddings_dict\n",
    "    embeddings_dict[image_id_string] = {\n",
    "        'patch_embeddings': patch_embeddings_output.cpu().numpy(),\n",
    "        'post_layer_norm': post_layer_norm_output.cpu().numpy(),\n",
    "        'q_former': q_former_output.cpu().numpy(),\n",
    "        'language_projection': language_projection_output.cpu().numpy(),\n",
    "    }\n",
    "\n",
    "    # remove hooks\n",
    "    patch_embeddings.remove()\n",
    "    post_layer_norm.remove()\n",
    "    language_projection.remove()\n",
    "    # mistral_model.remove()\n",
    "\n",
    "    return response\n",
    "\n",
    "prompt = \"Describe the findings\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/vol/biodata/data/chest_xray/VinDr-CXR/1.0.0_png_512/raw/test/f3426b51acbf433d03c5f84a5d16c0d3.png'), PosixPath('/vol/biodata/data/chest_xray/VinDr-CXR/1.0.0_png_512/raw/test/0b877b934aada3284ac13cfa74b5419c.png'), PosixPath('/vol/biodata/data/chest_xray/VinDr-CXR/1.0.0_png_512/raw/test/235fa5309a20f02f306297fee158a109.png'), PosixPath('/vol/biodata/data/chest_xray/VinDr-CXR/1.0.0_png_512/raw/test/6039745a7cf89b4f65da84eaa8c1a226.png'), PosixPath('/vol/biodata/data/chest_xray/VinDr-CXR/1.0.0_png_512/raw/test/0a8dc3de200bc767169b73a6ab91e948.png')]\n"
     ]
    }
   ],
   "source": [
    "vindr_dir = Path('/vol/biodata/data/chest_xray/VinDr-CXR/1.0.0_png_512/raw/test/')\n",
    "# list all images in the directory\n",
    "images = list(vindr_dir.glob('*.png'))\n",
    "# image = Image.open(image).convert(\"RGB\")\n",
    "print(images[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no pneumothorax or pleural effusion. There is no focal consolidation or pulmonary edema. The cardiomediastinal silhouette is within normal limits.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The heart size is normal. The mediastinal and hilar contours are normal. The pulmonary vasculature is normal. The lungs are clear. No pleural effusion or pneumothorax is seen.\n",
      "The lungs are hyperinflated with flattening of the diaphragms. There is no focal consolidation. There is no pleural effusion or pneumothorax. The cardiomediastinal silhouette is within normal limits.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for image in images[:3]:\n",
    "    response = generate_with_forward_hooks(image, prompt, processor, model, device, dtype, generation_config)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to save and clear the global lists to manage memory\n",
    "embeddings_dict = {}\n",
    "file_path = Path('/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/model_inspection/embeddings/VinDr_test_paired')\n",
    "file_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_embeddings_dict_pickle(embeddings_dict, batch_id, file_path):\n",
    "    file_path.mkdir(parents=True, exist_ok=True)\n",
    "    with open(file_path / f'embeddings_batch_{batch_id}.pkl', 'wb') as f:\n",
    "        pickle.dump(embeddings_dict, f)\n",
    "    embeddings_dict.clear()  # Clear the dictionary after saving\n",
    "\n",
    "batch_size = 100  # Choose an appropriate size\n",
    "num_images = len(images)\n",
    "num_batches = (num_images + batch_size - 1) // batch_size  # Ceiling division to account for the last batch\n",
    "\n",
    "for batch_id in range(num_batches):\n",
    "    batch_start = batch_id * batch_size\n",
    "    batch_end = min(batch_start + batch_size, num_images)\n",
    "    batch_images = images[batch_start:batch_end]\n",
    "    # generate the list of file names coreesponding to the images\n",
    "    file_names = [image.name for image in batch_images]\n",
    "    print(file_names)\n",
    "    for image in batch_images:\n",
    "        response = generate_with_forward_hooks(image, prompt, processor, model, device, dtype, generation_config)\n",
    "    \n",
    "    # After processing a batch, save the hook outputs and clear them from memory\n",
    "    save_embeddings_dict_pickle(embeddings_dict, batch_id, file_path)\n",
    "    # save_and_clear_hooks_output(batch_id,file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_dict)\n",
    "print(type(embeddings_dict['f3426b51acbf433d03c5f84a5d16c0d3']))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# compare the values of the embeddings in the pickle file and the embeddings_dict\n",
    "file_path = Path('/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/model_inspection/embeddings/VinDr_test_paired')\n",
    "file_path.mkdir(parents=True, exist_ok=True)\n",
    "for i in range(30):\n",
    "    print(f'Batch {i}')\n",
    "    with open(file_path / f'embeddings_batch_{i}.pkl', 'rb') as f:\n",
    "        embeddings_dict_pickled = pickle.load(f)\n",
    "\n",
    "        embeddings_dict_pickled_keys = embeddings_dict_pickled.keys()\n",
    "        # randomly select 10 keys from the embeddings_dict_pickled_keys and compare the values with generated with forward hooks\n",
    "        n = 25\n",
    "        random_keys = random.sample(embeddings_dict_pickled_keys, n)\n",
    "\n",
    "        for key in random_keys:\n",
    "            image = vindr_dir / f'{key}.png'\n",
    "            response = generate_with_forward_hooks(image, prompt, processor, model, device, dtype, generation_config)\n",
    "            if not np.allclose(embeddings_dict_pickled[key]['patch_embeddings'], patch_embeddings_output.cpu().numpy()):\n",
    "                print(f'Error in patch embeddings for {key}')\n",
    "            if not np.allclose(embeddings_dict_pickled[key]['post_layer_norm'], post_layer_norm_output.cpu().numpy()):\n",
    "                print(f'Error in post layer norm embeddings for {key}')\n",
    "            if not np.allclose(embeddings_dict_pickled[key]['q_former'], q_former_output.cpu().numpy()):\n",
    "                print(f'Error in q former embeddings for {key}')\n",
    "            if not np.allclose(embeddings_dict_pickled[key]['language_projection'], language_projection_output.cpu().numpy()):\n",
    "                print(f'Error in language projection embeddings for {key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 0\n",
    "\n",
    "# open the saved embeddings\n",
    "batch_patch_embeddings = torch.load(f'/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/model_inspection/embeddings/VinDr_test_paired/patch_embeddings_batch_{batch}.pt')\n",
    "batch_post_layer_norm_outputs = torch.load(f'/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/model_inspection/embeddings/VinDr_test_paired/post_layer_norm_outputs_batch_{batch}.pt')\n",
    "batch_q_former_outputs = torch.load(f'/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/model_inspection/embeddings/VinDr_test_paired/q_former_outputs_batch_{batch}.pt')\n",
    "batch_language_projection_outputs = torch.load(f'/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/model_inspection/embeddings/VinDr_test_paired/language_projection_outputs_batch_{batch}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print(len(batch_patch_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/.cache/modules/transformers_modules/StanfordAIMI/CheXagent-8b/4934e91451945c8218c267aae9c34929a7677829/processing_chexagent.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(pixel_values) for pixel_values in encoding_image_processor[\"pixel_values\"]]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for f3426b51acbf433d03c5f84a5d16c0d3.png\n",
      "Post layer norm outputs are not the same for f3426b51acbf433d03c5f84a5d16c0d3.png\n",
      "Q former outputs are not the same for f3426b51acbf433d03c5f84a5d16c0d3.png\n",
      "Language projection outputs are not the same for f3426b51acbf433d03c5f84a5d16c0d3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 0b877b934aada3284ac13cfa74b5419c.png\n",
      "Post layer norm outputs are not the same for 0b877b934aada3284ac13cfa74b5419c.png\n",
      "Q former outputs are not the same for 0b877b934aada3284ac13cfa74b5419c.png\n",
      "Language projection outputs are not the same for 0b877b934aada3284ac13cfa74b5419c.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 235fa5309a20f02f306297fee158a109.png\n",
      "Post layer norm outputs are not the same for 235fa5309a20f02f306297fee158a109.png\n",
      "Q former outputs are not the same for 235fa5309a20f02f306297fee158a109.png\n",
      "Language projection outputs are not the same for 235fa5309a20f02f306297fee158a109.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 6039745a7cf89b4f65da84eaa8c1a226.png\n",
      "Post layer norm outputs are not the same for 6039745a7cf89b4f65da84eaa8c1a226.png\n",
      "Q former outputs are not the same for 6039745a7cf89b4f65da84eaa8c1a226.png\n",
      "Language projection outputs are not the same for 6039745a7cf89b4f65da84eaa8c1a226.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 0a8dc3de200bc767169b73a6ab91e948.png\n",
      "Post layer norm outputs are not the same for 0a8dc3de200bc767169b73a6ab91e948.png\n",
      "Q former outputs are not the same for 0a8dc3de200bc767169b73a6ab91e948.png\n",
      "Language projection outputs are not the same for 0a8dc3de200bc767169b73a6ab91e948.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for ceec47e464c3dab60af04cd0a5122cf0.png\n",
      "Post layer norm outputs are not the same for ceec47e464c3dab60af04cd0a5122cf0.png\n",
      "Q former outputs are not the same for ceec47e464c3dab60af04cd0a5122cf0.png\n",
      "Language projection outputs are not the same for ceec47e464c3dab60af04cd0a5122cf0.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for e14840dd62dcaa3f2f429ed69a89059c.png\n",
      "Post layer norm outputs are not the same for e14840dd62dcaa3f2f429ed69a89059c.png\n",
      "Q former outputs are not the same for e14840dd62dcaa3f2f429ed69a89059c.png\n",
      "Language projection outputs are not the same for e14840dd62dcaa3f2f429ed69a89059c.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 0aa43b52e98700f8696bf36c10a76cea.png\n",
      "Post layer norm outputs are not the same for 0aa43b52e98700f8696bf36c10a76cea.png\n",
      "Q former outputs are not the same for 0aa43b52e98700f8696bf36c10a76cea.png\n",
      "Language projection outputs are not the same for 0aa43b52e98700f8696bf36c10a76cea.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for dba84e829f62c1272584f758b2204aa3.png\n",
      "Post layer norm outputs are not the same for dba84e829f62c1272584f758b2204aa3.png\n",
      "Q former outputs are not the same for dba84e829f62c1272584f758b2204aa3.png\n",
      "Language projection outputs are not the same for dba84e829f62c1272584f758b2204aa3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 5ef0d0b605f39b09df42d293e87971e3.png\n",
      "Post layer norm outputs are not the same for 5ef0d0b605f39b09df42d293e87971e3.png\n",
      "Q former outputs are not the same for 5ef0d0b605f39b09df42d293e87971e3.png\n",
      "Language projection outputs are not the same for 5ef0d0b605f39b09df42d293e87971e3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 0d8b5533f40cf4b0c3f2d86b1d61d6bc.png\n",
      "Post layer norm outputs are not the same for 0d8b5533f40cf4b0c3f2d86b1d61d6bc.png\n",
      "Q former outputs are not the same for 0d8b5533f40cf4b0c3f2d86b1d61d6bc.png\n",
      "Language projection outputs are not the same for 0d8b5533f40cf4b0c3f2d86b1d61d6bc.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for ae6434cc392e34dc83ce31ce480c5858.png\n",
      "Post layer norm outputs are not the same for ae6434cc392e34dc83ce31ce480c5858.png\n",
      "Q former outputs are not the same for ae6434cc392e34dc83ce31ce480c5858.png\n",
      "Language projection outputs are not the same for ae6434cc392e34dc83ce31ce480c5858.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for f2385e76586b911c47398381b3ab3dd3.png\n",
      "Post layer norm outputs are not the same for f2385e76586b911c47398381b3ab3dd3.png\n",
      "Q former outputs are not the same for f2385e76586b911c47398381b3ab3dd3.png\n",
      "Language projection outputs are not the same for f2385e76586b911c47398381b3ab3dd3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for f6a55d8e3471a4cce6e6be9713bf8acc.png\n",
      "Post layer norm outputs are not the same for f6a55d8e3471a4cce6e6be9713bf8acc.png\n",
      "Q former outputs are not the same for f6a55d8e3471a4cce6e6be9713bf8acc.png\n",
      "Language projection outputs are not the same for f6a55d8e3471a4cce6e6be9713bf8acc.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 4924e87f8069a1e4d3bb55f7bbeecdab.png\n",
      "Post layer norm outputs are not the same for 4924e87f8069a1e4d3bb55f7bbeecdab.png\n",
      "Q former outputs are not the same for 4924e87f8069a1e4d3bb55f7bbeecdab.png\n",
      "Language projection outputs are not the same for 4924e87f8069a1e4d3bb55f7bbeecdab.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for b922119aa88305502c4a33b7223b8931.png\n",
      "Post layer norm outputs are not the same for b922119aa88305502c4a33b7223b8931.png\n",
      "Q former outputs are not the same for b922119aa88305502c4a33b7223b8931.png\n",
      "Language projection outputs are not the same for b922119aa88305502c4a33b7223b8931.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for c9c96c529b368b1a26c6cadc143c5aae.png\n",
      "Post layer norm outputs are not the same for c9c96c529b368b1a26c6cadc143c5aae.png\n",
      "Q former outputs are not the same for c9c96c529b368b1a26c6cadc143c5aae.png\n",
      "Language projection outputs are not the same for c9c96c529b368b1a26c6cadc143c5aae.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 529e1b84a553a1eace23e78a49d041aa.png\n",
      "Post layer norm outputs are not the same for 529e1b84a553a1eace23e78a49d041aa.png\n",
      "Q former outputs are not the same for 529e1b84a553a1eace23e78a49d041aa.png\n",
      "Language projection outputs are not the same for 529e1b84a553a1eace23e78a49d041aa.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 3fe12c24bee9c864bf65441b49a53d79.png\n",
      "Post layer norm outputs are not the same for 3fe12c24bee9c864bf65441b49a53d79.png\n",
      "Q former outputs are not the same for 3fe12c24bee9c864bf65441b49a53d79.png\n",
      "Language projection outputs are not the same for 3fe12c24bee9c864bf65441b49a53d79.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 0af8628f5cbe0786db483a10934d1be5.png\n",
      "Post layer norm outputs are not the same for 0af8628f5cbe0786db483a10934d1be5.png\n",
      "Q former outputs are not the same for 0af8628f5cbe0786db483a10934d1be5.png\n",
      "Language projection outputs are not the same for 0af8628f5cbe0786db483a10934d1be5.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for a6f83640b41a64326aba96cdef3133dd.png\n",
      "Post layer norm outputs are not the same for a6f83640b41a64326aba96cdef3133dd.png\n",
      "Q former outputs are not the same for a6f83640b41a64326aba96cdef3133dd.png\n",
      "Language projection outputs are not the same for a6f83640b41a64326aba96cdef3133dd.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 925d945d9d6cf5d0f8bb294e8c68fdbc.png\n",
      "Post layer norm outputs are not the same for 925d945d9d6cf5d0f8bb294e8c68fdbc.png\n",
      "Q former outputs are not the same for 925d945d9d6cf5d0f8bb294e8c68fdbc.png\n",
      "Language projection outputs are not the same for 925d945d9d6cf5d0f8bb294e8c68fdbc.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for d025e3d642cb20b297bb70aa06eb3447.png\n",
      "Post layer norm outputs are not the same for d025e3d642cb20b297bb70aa06eb3447.png\n",
      "Q former outputs are not the same for d025e3d642cb20b297bb70aa06eb3447.png\n",
      "Language projection outputs are not the same for d025e3d642cb20b297bb70aa06eb3447.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 20a92721508c6b74dac74143d3221152.png\n",
      "Post layer norm outputs are not the same for 20a92721508c6b74dac74143d3221152.png\n",
      "Q former outputs are not the same for 20a92721508c6b74dac74143d3221152.png\n",
      "Language projection outputs are not the same for 20a92721508c6b74dac74143d3221152.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 884f2b574d9600dd52f4a04c9aa3a230.png\n",
      "Post layer norm outputs are not the same for 884f2b574d9600dd52f4a04c9aa3a230.png\n",
      "Q former outputs are not the same for 884f2b574d9600dd52f4a04c9aa3a230.png\n",
      "Language projection outputs are not the same for 884f2b574d9600dd52f4a04c9aa3a230.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for c4232411727c2821c17c059a260427d9.png\n",
      "Post layer norm outputs are not the same for c4232411727c2821c17c059a260427d9.png\n",
      "Q former outputs are not the same for c4232411727c2821c17c059a260427d9.png\n",
      "Language projection outputs are not the same for c4232411727c2821c17c059a260427d9.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for a7b95bc6823cdde758fc58a57885ca20.png\n",
      "Post layer norm outputs are not the same for a7b95bc6823cdde758fc58a57885ca20.png\n",
      "Q former outputs are not the same for a7b95bc6823cdde758fc58a57885ca20.png\n",
      "Language projection outputs are not the same for a7b95bc6823cdde758fc58a57885ca20.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 9e03f33a5f100880b2d2bdfbc46b7b72.png\n",
      "Post layer norm outputs are not the same for 9e03f33a5f100880b2d2bdfbc46b7b72.png\n",
      "Q former outputs are not the same for 9e03f33a5f100880b2d2bdfbc46b7b72.png\n",
      "Language projection outputs are not the same for 9e03f33a5f100880b2d2bdfbc46b7b72.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 4e685bcf518b823638340e2cb20fbad8.png\n",
      "Post layer norm outputs are not the same for 4e685bcf518b823638340e2cb20fbad8.png\n",
      "Q former outputs are not the same for 4e685bcf518b823638340e2cb20fbad8.png\n",
      "Language projection outputs are not the same for 4e685bcf518b823638340e2cb20fbad8.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 4e60eda03b7df9196e6309009114f54b.png\n",
      "Post layer norm outputs are not the same for 4e60eda03b7df9196e6309009114f54b.png\n",
      "Q former outputs are not the same for 4e60eda03b7df9196e6309009114f54b.png\n",
      "Language projection outputs are not the same for 4e60eda03b7df9196e6309009114f54b.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 4b4c014996678e36fa5aa1d74ff71a1a.png\n",
      "Post layer norm outputs are not the same for 4b4c014996678e36fa5aa1d74ff71a1a.png\n",
      "Q former outputs are not the same for 4b4c014996678e36fa5aa1d74ff71a1a.png\n",
      "Language projection outputs are not the same for 4b4c014996678e36fa5aa1d74ff71a1a.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for ab9dedb9ff4cd9e80dca74505b599105.png\n",
      "Post layer norm outputs are not the same for ab9dedb9ff4cd9e80dca74505b599105.png\n",
      "Q former outputs are not the same for ab9dedb9ff4cd9e80dca74505b599105.png\n",
      "Language projection outputs are not the same for ab9dedb9ff4cd9e80dca74505b599105.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for f599fe9d73fb3cc607a03fb258b7e198.png\n",
      "Post layer norm outputs are not the same for f599fe9d73fb3cc607a03fb258b7e198.png\n",
      "Q former outputs are not the same for f599fe9d73fb3cc607a03fb258b7e198.png\n",
      "Language projection outputs are not the same for f599fe9d73fb3cc607a03fb258b7e198.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for ca06f0a42e10816d50258f5cd663414a.png\n",
      "Post layer norm outputs are not the same for ca06f0a42e10816d50258f5cd663414a.png\n",
      "Q former outputs are not the same for ca06f0a42e10816d50258f5cd663414a.png\n",
      "Language projection outputs are not the same for ca06f0a42e10816d50258f5cd663414a.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for d6678cb7ae39f575d35ab9da6d7cb171.png\n",
      "Post layer norm outputs are not the same for d6678cb7ae39f575d35ab9da6d7cb171.png\n",
      "Q former outputs are not the same for d6678cb7ae39f575d35ab9da6d7cb171.png\n",
      "Language projection outputs are not the same for d6678cb7ae39f575d35ab9da6d7cb171.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 218e819ed8c0fe25c92ee7f1d5b993c7.png\n",
      "Post layer norm outputs are not the same for 218e819ed8c0fe25c92ee7f1d5b993c7.png\n",
      "Q former outputs are not the same for 218e819ed8c0fe25c92ee7f1d5b993c7.png\n",
      "Language projection outputs are not the same for 218e819ed8c0fe25c92ee7f1d5b993c7.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 6fe65e41cb661237e9030f191e3a2a9b.png\n",
      "Post layer norm outputs are not the same for 6fe65e41cb661237e9030f191e3a2a9b.png\n",
      "Q former outputs are not the same for 6fe65e41cb661237e9030f191e3a2a9b.png\n",
      "Language projection outputs are not the same for 6fe65e41cb661237e9030f191e3a2a9b.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 85ffcfa2cb02025abd57e2ed21ec4aa2.png\n",
      "Post layer norm outputs are not the same for 85ffcfa2cb02025abd57e2ed21ec4aa2.png\n",
      "Q former outputs are not the same for 85ffcfa2cb02025abd57e2ed21ec4aa2.png\n",
      "Language projection outputs are not the same for 85ffcfa2cb02025abd57e2ed21ec4aa2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 5d63f31cc8e21664c69bd7613b1b76e4.png\n",
      "Post layer norm outputs are not the same for 5d63f31cc8e21664c69bd7613b1b76e4.png\n",
      "Q former outputs are not the same for 5d63f31cc8e21664c69bd7613b1b76e4.png\n",
      "Language projection outputs are not the same for 5d63f31cc8e21664c69bd7613b1b76e4.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 5e9b9e927b853f5a95cd487b249e77b2.png\n",
      "Post layer norm outputs are not the same for 5e9b9e927b853f5a95cd487b249e77b2.png\n",
      "Q former outputs are not the same for 5e9b9e927b853f5a95cd487b249e77b2.png\n",
      "Language projection outputs are not the same for 5e9b9e927b853f5a95cd487b249e77b2.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for d2f46e1af0a1d93e1f8ec92633f1df86.png\n",
      "Post layer norm outputs are not the same for d2f46e1af0a1d93e1f8ec92633f1df86.png\n",
      "Q former outputs are not the same for d2f46e1af0a1d93e1f8ec92633f1df86.png\n",
      "Language projection outputs are not the same for d2f46e1af0a1d93e1f8ec92633f1df86.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 3cb7123b676ed1963d77850bfe185482.png\n",
      "Post layer norm outputs are not the same for 3cb7123b676ed1963d77850bfe185482.png\n",
      "Q former outputs are not the same for 3cb7123b676ed1963d77850bfe185482.png\n",
      "Language projection outputs are not the same for 3cb7123b676ed1963d77850bfe185482.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for ac5b9c5e3b65367ddbd1097db80f7a3c.png\n",
      "Post layer norm outputs are not the same for ac5b9c5e3b65367ddbd1097db80f7a3c.png\n",
      "Q former outputs are not the same for ac5b9c5e3b65367ddbd1097db80f7a3c.png\n",
      "Language projection outputs are not the same for ac5b9c5e3b65367ddbd1097db80f7a3c.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for be794e647d0f6f47a4cd21964c6f96e5.png\n",
      "Post layer norm outputs are not the same for be794e647d0f6f47a4cd21964c6f96e5.png\n",
      "Q former outputs are not the same for be794e647d0f6f47a4cd21964c6f96e5.png\n",
      "Language projection outputs are not the same for be794e647d0f6f47a4cd21964c6f96e5.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 96d1504f104b0b4ad88de4a50d413197.png\n",
      "Post layer norm outputs are not the same for 96d1504f104b0b4ad88de4a50d413197.png\n",
      "Q former outputs are not the same for 96d1504f104b0b4ad88de4a50d413197.png\n",
      "Language projection outputs are not the same for 96d1504f104b0b4ad88de4a50d413197.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for e78d3dafa3d35c1ef2beaed26f884b84.png\n",
      "Post layer norm outputs are not the same for e78d3dafa3d35c1ef2beaed26f884b84.png\n",
      "Q former outputs are not the same for e78d3dafa3d35c1ef2beaed26f884b84.png\n",
      "Language projection outputs are not the same for e78d3dafa3d35c1ef2beaed26f884b84.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for e1d0449f1cb674cecbd6c6dd3da50b1b.png\n",
      "Post layer norm outputs are not the same for e1d0449f1cb674cecbd6c6dd3da50b1b.png\n",
      "Q former outputs are not the same for e1d0449f1cb674cecbd6c6dd3da50b1b.png\n",
      "Language projection outputs are not the same for e1d0449f1cb674cecbd6c6dd3da50b1b.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 7d6d84f1373f54e8b4a7c72e4b0be842.png\n",
      "Post layer norm outputs are not the same for 7d6d84f1373f54e8b4a7c72e4b0be842.png\n",
      "Q former outputs are not the same for 7d6d84f1373f54e8b4a7c72e4b0be842.png\n",
      "Language projection outputs are not the same for 7d6d84f1373f54e8b4a7c72e4b0be842.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 75ef4701bba51b04b002e82495d7fc6b.png\n",
      "Post layer norm outputs are not the same for 75ef4701bba51b04b002e82495d7fc6b.png\n",
      "Q former outputs are not the same for 75ef4701bba51b04b002e82495d7fc6b.png\n",
      "Language projection outputs are not the same for 75ef4701bba51b04b002e82495d7fc6b.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 2f8f4f988fd84809caf191477ebc05db.png\n",
      "Post layer norm outputs are not the same for 2f8f4f988fd84809caf191477ebc05db.png\n",
      "Q former outputs are not the same for 2f8f4f988fd84809caf191477ebc05db.png\n",
      "Language projection outputs are not the same for 2f8f4f988fd84809caf191477ebc05db.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 0c0b8e98262a662c362929aa60e6bed9.png\n",
      "Post layer norm outputs are not the same for 0c0b8e98262a662c362929aa60e6bed9.png\n",
      "Q former outputs are not the same for 0c0b8e98262a662c362929aa60e6bed9.png\n",
      "Language projection outputs are not the same for 0c0b8e98262a662c362929aa60e6bed9.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 65279dbfdffd5d1250e8f7d0293d5727.png\n",
      "Post layer norm outputs are not the same for 65279dbfdffd5d1250e8f7d0293d5727.png\n",
      "Q former outputs are not the same for 65279dbfdffd5d1250e8f7d0293d5727.png\n",
      "Language projection outputs are not the same for 65279dbfdffd5d1250e8f7d0293d5727.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 35140570aec20f94b22429d00dc142e9.png\n",
      "Post layer norm outputs are not the same for 35140570aec20f94b22429d00dc142e9.png\n",
      "Q former outputs are not the same for 35140570aec20f94b22429d00dc142e9.png\n",
      "Language projection outputs are not the same for 35140570aec20f94b22429d00dc142e9.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 5175798e4a23ac250b47c0c4fd83a725.png\n",
      "Post layer norm outputs are not the same for 5175798e4a23ac250b47c0c4fd83a725.png\n",
      "Q former outputs are not the same for 5175798e4a23ac250b47c0c4fd83a725.png\n",
      "Language projection outputs are not the same for 5175798e4a23ac250b47c0c4fd83a725.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 10d3204872c83731aeaee57e020b768a.png\n",
      "Post layer norm outputs are not the same for 10d3204872c83731aeaee57e020b768a.png\n",
      "Q former outputs are not the same for 10d3204872c83731aeaee57e020b768a.png\n",
      "Language projection outputs are not the same for 10d3204872c83731aeaee57e020b768a.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 3f311e13697f83d41f55f079232fe7ee.png\n",
      "Post layer norm outputs are not the same for 3f311e13697f83d41f55f079232fe7ee.png\n",
      "Q former outputs are not the same for 3f311e13697f83d41f55f079232fe7ee.png\n",
      "Language projection outputs are not the same for 3f311e13697f83d41f55f079232fe7ee.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 53ae3d1a464529cb387ce44aa655846f.png\n",
      "Post layer norm outputs are not the same for 53ae3d1a464529cb387ce44aa655846f.png\n",
      "Q former outputs are not the same for 53ae3d1a464529cb387ce44aa655846f.png\n",
      "Language projection outputs are not the same for 53ae3d1a464529cb387ce44aa655846f.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for e52b5e6afc1c45823626dfc0b0c07a8c.png\n",
      "Post layer norm outputs are not the same for e52b5e6afc1c45823626dfc0b0c07a8c.png\n",
      "Q former outputs are not the same for e52b5e6afc1c45823626dfc0b0c07a8c.png\n",
      "Language projection outputs are not the same for e52b5e6afc1c45823626dfc0b0c07a8c.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 2f629036ede0886b8a4eef89f6526fc6.png\n",
      "Post layer norm outputs are not the same for 2f629036ede0886b8a4eef89f6526fc6.png\n",
      "Q former outputs are not the same for 2f629036ede0886b8a4eef89f6526fc6.png\n",
      "Language projection outputs are not the same for 2f629036ede0886b8a4eef89f6526fc6.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 5995bef3891e94a9666695e1302247cc.png\n",
      "Post layer norm outputs are not the same for 5995bef3891e94a9666695e1302247cc.png\n",
      "Q former outputs are not the same for 5995bef3891e94a9666695e1302247cc.png\n",
      "Language projection outputs are not the same for 5995bef3891e94a9666695e1302247cc.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 32ce0ed98d68f9ed492aa9cdbe8a11fe.png\n",
      "Post layer norm outputs are not the same for 32ce0ed98d68f9ed492aa9cdbe8a11fe.png\n",
      "Q former outputs are not the same for 32ce0ed98d68f9ed492aa9cdbe8a11fe.png\n",
      "Language projection outputs are not the same for 32ce0ed98d68f9ed492aa9cdbe8a11fe.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 5beee832c007826c93c25ba287ee4d48.png\n",
      "Post layer norm outputs are not the same for 5beee832c007826c93c25ba287ee4d48.png\n",
      "Q former outputs are not the same for 5beee832c007826c93c25ba287ee4d48.png\n",
      "Language projection outputs are not the same for 5beee832c007826c93c25ba287ee4d48.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 950f7420fe9cb8a960a5c3ca47c35748.png\n",
      "Post layer norm outputs are not the same for 950f7420fe9cb8a960a5c3ca47c35748.png\n",
      "Q former outputs are not the same for 950f7420fe9cb8a960a5c3ca47c35748.png\n",
      "Language projection outputs are not the same for 950f7420fe9cb8a960a5c3ca47c35748.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 91283908618845879937985817083c93.png\n",
      "Post layer norm outputs are not the same for 91283908618845879937985817083c93.png\n",
      "Q former outputs are not the same for 91283908618845879937985817083c93.png\n",
      "Language projection outputs are not the same for 91283908618845879937985817083c93.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for d2e77def0302c94f744079974be9bc53.png\n",
      "Post layer norm outputs are not the same for d2e77def0302c94f744079974be9bc53.png\n",
      "Q former outputs are not the same for d2e77def0302c94f744079974be9bc53.png\n",
      "Language projection outputs are not the same for d2e77def0302c94f744079974be9bc53.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for dd8e8db5d12ef22a32ee27794dfa0d17.png\n",
      "Post layer norm outputs are not the same for dd8e8db5d12ef22a32ee27794dfa0d17.png\n",
      "Q former outputs are not the same for dd8e8db5d12ef22a32ee27794dfa0d17.png\n",
      "Language projection outputs are not the same for dd8e8db5d12ef22a32ee27794dfa0d17.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for f3699d3679128726466b307f7a7cba73.png\n",
      "Post layer norm outputs are not the same for f3699d3679128726466b307f7a7cba73.png\n",
      "Q former outputs are not the same for f3699d3679128726466b307f7a7cba73.png\n",
      "Language projection outputs are not the same for f3699d3679128726466b307f7a7cba73.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 6cce56dda2a5b90c9ac2987b1a8e3f45.png\n",
      "Post layer norm outputs are not the same for 6cce56dda2a5b90c9ac2987b1a8e3f45.png\n",
      "Q former outputs are not the same for 6cce56dda2a5b90c9ac2987b1a8e3f45.png\n",
      "Language projection outputs are not the same for 6cce56dda2a5b90c9ac2987b1a8e3f45.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 119a7830be7eaba5f4feb67d1ee9011e.png\n",
      "Post layer norm outputs are not the same for 119a7830be7eaba5f4feb67d1ee9011e.png\n",
      "Q former outputs are not the same for 119a7830be7eaba5f4feb67d1ee9011e.png\n",
      "Language projection outputs are not the same for 119a7830be7eaba5f4feb67d1ee9011e.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 0e5f2fdf49cbfdbe2a7c0861d15a73ff.png\n",
      "Post layer norm outputs are not the same for 0e5f2fdf49cbfdbe2a7c0861d15a73ff.png\n",
      "Q former outputs are not the same for 0e5f2fdf49cbfdbe2a7c0861d15a73ff.png\n",
      "Language projection outputs are not the same for 0e5f2fdf49cbfdbe2a7c0861d15a73ff.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for fb1150a90723df92e2385c956db5aeef.png\n",
      "Post layer norm outputs are not the same for fb1150a90723df92e2385c956db5aeef.png\n",
      "Q former outputs are not the same for fb1150a90723df92e2385c956db5aeef.png\n",
      "Language projection outputs are not the same for fb1150a90723df92e2385c956db5aeef.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for 40cd10b8770cee353b0ae6a8192097ab.png\n",
      "Post layer norm outputs are not the same for 40cd10b8770cee353b0ae6a8192097ab.png\n",
      "Q former outputs are not the same for 40cd10b8770cee353b0ae6a8192097ab.png\n",
      "Language projection outputs are not the same for 40cd10b8770cee353b0ae6a8192097ab.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for f5de7e513d2c4085e8b7fd6b5e5db57a.png\n",
      "Post layer norm outputs are not the same for f5de7e513d2c4085e8b7fd6b5e5db57a.png\n",
      "Q former outputs are not the same for f5de7e513d2c4085e8b7fd6b5e5db57a.png\n",
      "Language projection outputs are not the same for f5de7e513d2c4085e8b7fd6b5e5db57a.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings are not the same for c6829051ecb281656c987fa2cfe5c706.png\n",
      "Post layer norm outputs are not the same for c6829051ecb281656c987fa2cfe5c706.png\n",
      "Q former outputs are not the same for c6829051ecb281656c987fa2cfe5c706.png\n",
      "Language projection outputs are not the same for c6829051ecb281656c987fa2cfe5c706.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m language_projection_outputs\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     13\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/vol/biodata/data/chest_xray/VinDr-CXR/1.0.0_png_512/raw/test/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_with_forward_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# check if the embeddings are the same\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(patch_embeddings_outputs[\u001b[38;5;241m0\u001b[39m], batch_patch_embeddings[index]):\n",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m, in \u001b[0;36mgenerate_with_forward_hooks\u001b[0;34m(images, prompt, processor, model, device, dtype, generation_config)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# mistral_model = model.language_model.model.norm.register_forward_hook(mistral_model_output_hook)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# complete a forward pass \u001b[39;00m\n\u001b[1;32m     10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(\n\u001b[1;32m     11\u001b[0m     images\u001b[38;5;241m=\u001b[39mimages, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m USER: <s>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ASSISTANT: <s>\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m---> 13\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     14\u001b[0m response \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# remove hooks\u001b[39;00m\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/.cache/modules/transformers_modules/StanfordAIMI/CheXagent-8b/4934e91451945c8218c267aae9c34929a7677829/modeling_chexagent.py:1295\u001b[0m, in \u001b[0;36mCheXagentForConditionalGeneration.generate\u001b[0;34m(self, pixel_values, input_ids, attention_mask, **generate_kwargs)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m inputs_lang\n\u001b[1;32m   1293\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m lang_atts\n\u001b[0;32m-> 1295\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1558\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1552\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1553\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1554\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1555\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1556\u001b[0m     )\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE:\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1573\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2995\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2992\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m next_tokens \u001b[38;5;241m%\u001b[39m vocab_size\n\u001b[1;32m   2994\u001b[0m \u001b[38;5;66;03m# stateless\u001b[39;00m\n\u001b[0;32m-> 2995\u001b[0m beam_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_scorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_token_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3001\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3004\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3006\u001b[0m beam_scores \u001b[38;5;241m=\u001b[39m beam_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_beam_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3007\u001b[0m beam_next_tokens \u001b[38;5;241m=\u001b[39m beam_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_beam_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/.venv/lib/python3.10/site-packages/transformers/generation/beam_search.py:253\u001b[0m, in \u001b[0;36mBeamSearchScorer.process\u001b[0;34m(self, input_ids, next_scores, next_tokens, next_indices, pad_token_id, eos_token_id, beam_indices, group_index, decoder_prompt_len)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m    252\u001b[0m     batch_group_idx \u001b[38;5;241m=\u001b[39m batch_idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_beam_groups \u001b[38;5;241m+\u001b[39m group_index\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_done[batch_group_idx]:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_beams \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_hyps[batch_group_idx]):\n\u001b[1;32m    255\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch can only be done if at least \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_beams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m beams have been generated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(f'/vol/biomedic3/bglocker/ugproj2324/nns20/CheXagent/model_inspection/embeddings/VinDr_test/file_names_batch_{batch_id}.txt', 'r') as f:\n",
    "    file_names = f.readlines()\n",
    "    file_names = [file_name.strip() for file_name in file_names]\n",
    "\n",
    "\n",
    "for index, file in enumerate(file_names):\n",
    "    # clear the lists\n",
    "    patch_embeddings_outputs.clear()\n",
    "    post_layer_norm_outputs.clear()\n",
    "    q_former_outputs.clear()\n",
    "    language_projection_outputs.clear()\n",
    "\n",
    "    image = Image.open(f'/vol/biodata/data/chest_xray/VinDr-CXR/1.0.0_png_512/raw/test/{file}').convert(\"RGB\")\n",
    "    response = generate_with_forward_hooks(image, prompt, processor, model, device, dtype, generation_config)\n",
    "    # check if the embeddings are the same\n",
    "    if not torch.allclose(patch_embeddings_outputs[0], batch_patch_embeddings[index]):\n",
    "        print(f\"Patch embeddings are not the same for {file}\")\n",
    "    \n",
    "    if not torch.allclose(post_layer_norm_outputs[0], batch_post_layer_norm_outputs[index]):\n",
    "        print(f\"Post layer norm outputs are not the same for {file}\")\n",
    "        \n",
    "    if not torch.allclose(q_former_outputs[0], batch_q_former_outputs[index]):\n",
    "        print(f\"Q former outputs are not the same for {file}\")\n",
    "    \n",
    "    if not torch.allclose(language_projection_outputs[0], batch_language_projection_outputs[index]):\n",
    "        print(f\"Language projection outputs are not the same for {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# compare the embeddings\n",
    "# patch embeddings\n",
    "print(torch.allclose(batch_0_patch_embeddings[0], patch_embeddings_outputs[0]))\n",
    "# post layer norm\n",
    "print(torch.allclose(batch_0_post_layer_norm_outputs[0], post_layer_norm_outputs[0]))\n",
    "# q_former\n",
    "print(torch.allclose(batch_0_q_former_outputs[0], q_former_outputs[0]))\n",
    "# language projection\n",
    "print(torch.allclose(batch_0_language_projection_outputs[0], language_projection_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2561, -0.2659,  1.2744,  ...,  0.2036, -0.5537, -1.1152],\n",
      "         [-0.2566, -0.2654,  1.2744,  ...,  0.2036, -0.5542, -1.1143],\n",
      "         [-0.2559, -0.2646,  1.2734,  ...,  0.2040, -0.5532, -1.1133],\n",
      "         ...,\n",
      "         [-0.2563, -0.2651,  1.2744,  ...,  0.2040, -0.5527, -1.1143],\n",
      "         [-0.2908, -0.2450,  1.2832,  ...,  0.1680, -0.4976, -1.1416],\n",
      "         [-0.2561, -0.2656,  1.2744,  ...,  0.2040, -0.5537, -1.1152]]],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(batch_0_q_former_outputs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
